# -*- coding: utf-8 -*-
"""hateXplain_lime_explanation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gKSPUe-eVA_fkumV5N16dEnC014P5gtj

# Colab installation
"""

# Install transformer and hugging face


"""# Header file"""

import os 
import math

import string
import re
import pandas as pd
import json
import psutil
import time
import emoji
from tqdm import tqdm

import pandas as pd
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import Dataset, random_split, TensorDataset, DataLoader, RandomSampler, SequentialSampler

import transformers
from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, AdamW, RobertaTokenizer, RobertaModel

from transformers import XLMRobertaTokenizer, XLMRobertaModel


from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rc
import networkx as nx
from networkx.drawing.nx_agraph import write_dot, graphviz_layout
import torch.nn.functional as F
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from collections import Counter

import time, datetime, pytz

"""### Cache location

# Variables
"""

IST = pytz.timezone('Asia/Kolkata') # your time zone here

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Training on {device}')


import argparse, sys

parser = argparse.ArgumentParser(description='to generate LRP & LIME scores on test_data in ERASER format')
parser.add_argument('--method', type=str, help='method if lrp then enter 0/1/2, if lime then enter "lime"')
parser.add_argument('--faithfullness_filtering', type=str, help='top-k tokens or above a threshold, top-k/0.5', default='top-k')
parser.add_argument('--split', type=int, help='data split, 1/2/3')
parser.add_argument('--model_path', type=str, help='path of model checkpoints using which LRP/LIME scores needed to calculate')
parser.add_argument('--data_path', type=str, help='data path')
parser.add_argument('--encoder_name', type=str, help='name of encoder, bert-base-cased/xlm-roberta-base', default='bert-base-cased')
parser.add_argument('--drop_out', type=float, default=0.4)
parser.add_argument('--encoder_frozen', type=str, help='encoder layers frozen or not', default='False')

args = parser.parse_args()

rid = re.search(r'\d+',args.model_path)[0]
report_path = 'reports/'+'run_ID_'+str(rid)+'_classification_report.txt'
with open(report_path, 'a') as f:
    f.write('======= hatexplain metrics on: '+args.model_path+'==========\n')
    for sa in sys.argv:
        f.write(sa+' ')
    f.write('\nBegin '+datetime.datetime.now(IST).strftime("%c")+'\n')

# randon seed for model or randon function
SEED = 63571
import os, random
os.environ['PYTHONHASHSEED'] = str(SEED)
# Torch RNG
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
# Python RNG
np.random.seed(SEED)
random.seed(SEED)

# language
language = "hi"

# Parameters for dataset
dataset_parameter = {
                     'header' : 0,
                     'batch_size' : 32,
                     'max_seq_len' : 100,
                     'dataset_split' : [0.7, 0.15, 0.15],
                     'sentence_column' :   'text',         #'commentText',
                     'label_column' : 'label',            #'label',
                     'num_classes':2
                    }


# transformer model parameters

bert_model_parameter = {'name' : 'XLM-Roberta',
                   'hugging_face_name' : args.encoder_name, #'xlm-roberta-base',
                   'tokenizer' : args.encoder_name, #'xlm-roberta-base',
                   'second_last_layer_size' : 768,
                   'config' : None,
                   'sub_word_starting_char': '‚ñÅ',
                   'tokenizer_cls_id': None,
                   'tokenizer_sep_id': None,
                   'tokenizer_pad_id': None,
                   'token_ids_way': 3,
                   'word_level_mean_way' : 5,
                   }

model_parameter = bert_model_parameter

# Optimizer, loss function and other training paramters
training_parameter = {
                 
                      'hidden_layers'     : [128, 32],
                      'activation_function'   : 'leaky_relu',  #choose one out of ['relu', 'prelu', 'elu', 'leaky_relu']
                      'optimizer_parameter' : {'name' : 'AdamW',
                                               'lr' : 1e-5},
                      'loss_func_parameter' : {'name' : 'NLLLoss',
                                               'class_weight' : [1, 1]},
                      'epoch' : 10,
                     }

"""# Data Preprocessor"""

def remove_punctuation(text):
    table = str.maketrans("", "", string.punctuation)
    return text.translate(table)

def remove_extra_space(text):
    return re.sub(' +', ' ', text)

def remove_username(text):
    return re.sub('@[\w]+','',text)

def remove_mentions(text):
    return re.sub('#[\w]+','',text)

def remove_url(text):
    return re.sub(('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'), '', text)

def remove_rt(text):
    return re.sub('RT :','',text)

def remove_newline(text):
    return re.sub('(\r|\n|\t)','',text)

import demoji

def remove_emoji(text):
    return demoji.replace(text, repl="")

def remove_emoji2(text):
    regrex_pattern = re.compile("["
                                u"\U0001F600-\U0001F64F"  # emoticons
                                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                u"\U0001F1F2-\U0001F1F4"  # Macau flag
                                u"\U0001F1E6-\U0001F1FF"  # flags
                                u"\U0001F600-\U0001F64F"
                                u"\U00002702-\U000027B0"
                                u"\U000024C2-\U0001F251"
                                u"\U0001f926-\U0001f937"
                                u"\U0001F1F2"
                                u"\U0001F1F4"
                                u"\U0001F620"
                                u"\u200c"
                                u"\u200d"
                                u"\u2640-\u2642"
                                "]+", flags=re.UNICODE)
    return regrex_pattern.sub(r'',text)



def remove_html_tags(text):
    """Remove html tags from a string"""
    import re
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def remove_dots_from_shortforms(text):
    text = text.split('.') #t.v --> tv
    return ("").join(text)

def remove_special_char(text):
    return re.sub('\W+',' ', text)

def preprocess_text(text):
    # text = remove_emoji(text) # not including remove_emoji() in preprocessing as ROBERTa can handle emojis too..   
    text = remove_username(text)
    text = remove_emoji(text)
    text = remove_emoji2(text)
    text = remove_html_tags(text).strip()
    text = remove_rt(text)
    text = remove_url(text)
    text = remove_dots_from_shortforms(text)
    text = remove_special_char(text)
    text = text.lower()
    text = remove_mentions(text) 
    text = remove_newline(text)
    text = remove_extra_space(text)
    return text

"""# Model and tokenizer import"""



tokenizer = AutoTokenizer.from_pretrained(bert_model_parameter['tokenizer'])
bert = AutoModel.from_pretrained(bert_model_parameter['hugging_face_name'])

bert_model_parameter['tokenizer_cls_id'], _, bert_model_parameter['tokenizer_sep_id'], bert_model_parameter['tokenizer_pad_id'] = tokenizer("i", return_tensors="pt", max_length=4, padding='max_length')['input_ids'][0].tolist()

def tokenizer_word_length(text):
    """
        This function will calculate length of input ids from tokenizer
            
        Input ids are the token ids which are calculated from tokenizers.
        Here we are calculate ids of each word and then appending it in the final ids list, 
        in the mean if user want to include only first or last or all ids then this can be done with varible model_parameter['token_ids_way']
                    
        text: (list of list) list of text, first dimension is sample size and second is string or text
            
        return: (1D list) length of input ids
    """
    tokenized_len = []
    
    for row in text:
        space_seq_text = row.split(" ")
        text_input_ids = [bert_model_parameter['tokenizer_cls_id']]        
        for word in space_seq_text:
            word_token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))
            text_input_ids += word_token_ids
        text_input_ids += [bert_model_parameter['tokenizer_sep_id']]
        tokenized_len.append(len(text_input_ids))
        
    return tokenized_len


# Ritwik's tokenize_word function

import stanza
import pandas as pd
from transformers import AutoTokenizer

nlp = stanza.Pipeline('en',processors='tokenize', download_method=2) # put desired language here
# stanza.Pipeline('en',download_method=2)
add_cls_sep_token_ids = True


def filter_for_max_len(sent_list, label_list):
    sents2, labels2 = [], []
    for s, l in tqdm(zip(sent_list, label_list), total=len(sent_list), ncols=150, desc='filtering'):
        tokenized_text_i = []
        doc = nlp(s)
        for sents in doc.sentences:
            for word in sents.words:
                tokenized_text_i.append(word.text)
        s = ' '.join(tokenized_text_i)
        if len(tokenizer(s,return_tensors="pt")['input_ids'][0]) <= dataset_parameter['max_seq_len']:
            sents2.append(s)
            labels2.append(l)
    return sents2, labels2

def tokenize_word_ritwik(text):
    input_ids, attention_mask, word_break, word_break_len, tag_one_hot, tag_attention = [],[],[],[], [] , []

    # assert len(text) == len(tags)
    # onehot_encoding = pd.get_dummies(dataset_parameter['tag_set'])
    for text_i in text:
        input_ids_i, attention_mask_i, word_break_i, word_break_len_i, tag1hot_i, tag_att_i = [model_parameter['tokenizer_cls_id']] if add_cls_sep_token_ids else [], [], [], 0, [], [0] if add_cls_sep_token_ids else []

        # tokenized_text_i = []
        # doc = nlp(text_i)
        # for sents in doc.sentences:
        # 	for word in sents.words:
        # 		tokenized_text_i.append(word.text)
        tokenized_text_i = text_i.split() # since humne pehle filtering kardi hai, yahan pe dobara stanza se pass karne ki zarurat ni hai

        for i, word in enumerate(tokenized_text_i):
            input_ids_per_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))
     
            if len(input_ids_per_word) > 1:
                if model_parameter['token_ids_way']==1:
                    input_ids_per_word = [input_ids_per_word[0]]
                    word_break_i.append(1)
                elif model_parameter['token_ids_way']==2:
                    input_ids_per_word = [input_ids_per_word[-1]]
                    word_break_i.append(1)
                elif model_parameter['token_ids_way']==3:
                    word_break_i.append(len(input_ids_per_word))
                else:
                    raise "Error ajeeb"
            else:
                word_break_i.append(1)
            input_ids_i += input_ids_per_word

            # tag1hot_i.append(onehot_encoding[tag].tolist())
            # tag_att_i.append(1)

        # -------------- truncation start --------------

        input_ids_i = input_ids_i[:dataset_parameter['max_seq_len']+ (1 if add_cls_sep_token_ids else 0)] # notice I added 1 to input ids only because baaki saari lists empty thi initially whereas input ids wali list empty nhi thi i.e. it contained cls token id
        word_break_i = word_break_i[:dataset_parameter['max_seq_len']]
        word_break_len_i = len(word_break_i) # number of words
        # --------------  truncation end  --------------
        # print(input_ids_i)
        # input()
        if add_cls_sep_token_ids:
            input_ids_i.append(model_parameter['tokenizer_sep_id'])
        attention_mask_i += [1]*len(input_ids_i)

        # # -------------- padding start --------------
        input_ids_i = input_ids_i + [model_parameter['tokenizer_pad_id']]*(dataset_parameter['max_seq_len']+(2 if add_cls_sep_token_ids else 0) - len(input_ids_i))
        attention_mask_i = attention_mask_i + [0]*(dataset_parameter['max_seq_len']+(2 if add_cls_sep_token_ids else 0) - len(attention_mask_i))
        word_break_i = word_break_i + [0]*(dataset_parameter['max_seq_len'] - len(word_break_i))
    
        # # --------------  padding end  --------------

        input_ids.append(input_ids_i)
        attention_mask.append(attention_mask_i)
        word_break.append(word_break_i)
        word_break_len.append(word_break_len_i)

 
    # print('attention_mask.shape: ', attention_mask.shape)

 
    return input_ids, attention_mask, word_break, word_break_len



#this is for model.predict() fn only!
def tokenize_word_ritwik2(text):
    input_ids, attention_mask, word_break, word_break_len, tag_one_hot, tag_attention = [],[],[],[], [] , []

    # assert len(text) == len(tags)
    # onehot_encoding = pd.get_dummies(dataset_parameter['tag_set'])
    for text_i in text:
        input_ids_i, attention_mask_i, word_break_i, word_break_len_i, tag1hot_i, tag_att_i = [model_parameter['tokenizer_cls_id']] if add_cls_sep_token_ids else [], [], [], 0, [], [0] if add_cls_sep_token_ids else []

        # tokenized_text_i = []
        # doc = nlp(text_i)
        # for sents in doc.sentences:
        # 	for word in sents.words:
        # 		tokenized_text_i.append(word.text)
        tokenized_text_i = text_i.split() # since humne pehle filtering kardi hai, yahan pe dobara stanza se pass karne ki zarurat ni hai

        for i, word in enumerate(tokenized_text_i):
            input_ids_per_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))
     
            if len(input_ids_per_word) > 1:
                if model_parameter['token_ids_way']==1:
                    input_ids_per_word = [input_ids_per_word[0]]
                    word_break_i.append(1)
                elif model_parameter['token_ids_way']==2:
                    input_ids_per_word = [input_ids_per_word[-1]]
                    word_break_i.append(1)
                elif model_parameter['token_ids_way']==3:
                    word_break_i.append(len(input_ids_per_word))
                else:
                    raise "Error ajeeb"
            else:
                word_break_i.append(1)
            input_ids_i += input_ids_per_word

            # tag1hot_i.append(onehot_encoding[tag].tolist())
            # tag_att_i.append(1)

        # -------------- truncation start --------------
        #input_ids_i = input_ids_i[:dataset_parameter['max_seq_len']+ (1 if add_cls_sep_token_ids else 0)] # notice I added 1 to input ids only because baaki saari lists empty thi initially whereas input ids wali list empty nhi thi i.e. it contained cls token id
        word_break_i = word_break_i[:dataset_parameter['max_seq_len']]
        word_break_len_i = len(word_break_i) # number of words
        # --------------  truncation end  --------------
        # print(input_ids_i)
        # input()
        if add_cls_sep_token_ids:
            input_ids_i.append(model_parameter['tokenizer_sep_id'])
        attention_mask_i += [1]*len(input_ids_i)

        # # -------------- padding start --------------
        # input_ids_i = input_ids_i + [model_parameter['tokenizer_pad_id']]*(dataset_parameter['max_seq_len']+(2 if add_cls_sep_token_ids else 0) - len(input_ids_i))
        # attention_mask_i = attention_mask_i + [0]*(dataset_parameter['max_seq_len']+(2 if add_cls_sep_token_ids else 0) - len(attention_mask_i))
        # word_break_i = word_break_i + [0]*(dataset_parameter['max_seq_len'] - len(word_break_i))
    
        # # --------------  padding end  --------------

        input_ids.append(input_ids_i)
        attention_mask.append(attention_mask_i)
        word_break.append(word_break_i)
        word_break_len.append(word_break_len_i)

    return input_ids, attention_mask, word_break, word_break_len


"""# Dataloader"""

class Dataset_loader(Dataset):
    def __init__(self):
        """
            Load 
                - preprocessed data from "Raw data" and "Stanza library" section and                 
                - adjacency matrix made from dependency relation from stanza
            Process
                - tokenize the dataset into encoding, mask and labels
                - word break list and its length
                - tensor dataloader of all variables/features
            
            Max length is +2 in tokenizer because tokenizer will pad start and ending of text, we are only considering length of sentace

            !!! In-future all data related functions will be integrated in the dataloader class !!!
            return: None
        """
        # df = pd.read_csv('/content/drive/MyDrive/MIDAS/LRP/IIITD-kaggle-chllng/train-data-kaggle-challenge.csv')
        # df = df[df['language']=='Hindi']

        # df = pd.read_csv('/content/drive/MyDrive/MIDAS/LRP/olid-training-v1.0.tsv', sep= '\t', error_bad_lines=False)
        # df['subtask_a'] = df['subtask_a'].apply(lambda label : 1 if label=='OFF' else 0)

        # df  = pd.read_csv("/content/drive/MyDrive/MIDAS/LRP/dataset_2_8_15_combined.csv")

        #hatexplain dataset 
        df = pd.read_json('/content/drive/MyDrive/MIDAS/LRP/hateXplainDataset.json')
        df = df.T



        with open(data_file_path) as fp:
            data = json.load(fp)

        keep_ids = []
        for ele in data:
            keep_ids.append(ele['annotation_id'])


        df = df[df["post_id"].isin(keep_ids)]

        df['text'] = df['post_tokens'].apply(lambda l : " ".join(l))
        df['label_list'] = df['annotators'].apply(lambda ele : [d['label'] for d in ele])
        df['final_label'] = df['label_list'].apply(lambda label_list : max(label_list,key=label_list.count))
        df['label'] = df['final_label'].apply(lambda label : 0 if label=='normal' else 1)



        df = df[[dataset_parameter['sentence_column'], dataset_parameter['label_column']]]
        df[dataset_parameter['sentence_column']] = df[dataset_parameter['sentence_column']].apply(lambda x: preprocess_text(x))


        df['length'] = tokenizer_word_length(df[dataset_parameter['sentence_column']])
        df = df[df.length <dataset_parameter['max_seq_len']]
        
        text = list(df.iloc[:,0])
        label = list(df.iloc[:,1])


        text, label = filter_for_max_len(text, label)

        
        # calculate input, attention , word break and word break length from tokenize word function
        # df['input_ids'], df['attention_mask'], df['word_break'], df['word_break_len'] = tokenize_word1(text)

        input_ids, attention_mask, word_break, word_break_len = tokenize_word_ritwik(text)
        # input_ids2, attention_mask2, word_break2, word_break_len2 = tokenize_word2(text)



        # df['input_ids'], df['attention_mask'], df['word_break'], df['word_break_len'] = input_ids1, attention_mask1, word_break1, word_break_len1



        # get tokenizer input id, attention mask, word_break and word break len
        # input_ids, attention_mask, word_break, word_break_len = list(df['input_ids']), list(df['attention_mask']), list(df['word_break']), list(df['word_break_len'])


        # to preprocess data and convert all data to torch tensor
        input_ids = torch.tensor(input_ids)
        attention_mask = torch.tensor(attention_mask)
        word_break = torch.tensor(word_break)
        word_break_len = torch.tensor(word_break_len)
        label = torch.tensor(label)
                        
        self.length = len(label)        
        self.dataset = TensorDataset(input_ids, attention_mask, word_break, word_break_len, label)
        
        
    def __len__(self):
        """
            This will return length of dataset
            
            return: (int) length of dataset 
        """
        return self.length

    def __getitem__(self, id):
        """
            Give encoding, mark and label at ith index given by user as id
            
            id: (int) 
            
            return: (list) custom vector at specified index (vector, mask, labels)
        """
        return self.dataset[id] 


def word_break_list(text):
    """
        Generate word break array if nth word in text is break into k piece then nth element 
        in resultant array with be k otherwise 1
        
        For example text is "He cannot be trusted", 
        tokenized text will be "He can ##not be trust ##ed", cannot & trusted broke into 2 sub words
        Its word breakage list will be [1, 2, 1, 2]
        
        dataset: (str) Single text/sentence
        
        return: (list) word breakage list
    """
    tokenized_text =    tokenizer(text, 
                                    max_length=dataset_parameter['max_seq_len'],
                                    padding=True,
                                    truncation=True)
    breakage = []
    counter=0
    for token in tokenized_text:
        #tokenized word have '_' in starting
        if token[:2]!="_":     
            counter+=1
        breakage.append(counter)
    freq = {}
    for i in breakage:
        if i in freq:
            freq[i] = freq[i]+1
        else:
            freq[i] = 1
    freq = list(freq.values())
    return freq    

def dataloader_creator(dataset):
    """
        Generate dataloader of torch class with batch size and sampler defined
        
        dataset: (torch.data.Dataset class) Dataset on which dataloader will be created
        
        return: (torch.data.Dataloader)
    """
    dataset_sampler = RandomSampler(dataset) 
    dataset_dataloader = DataLoader(  dataset=dataset, 
                                      sampler=dataset_sampler, 
                                      batch_size=dataset_parameter['batch_size'])
    return dataset_dataloader
    
def dataloader_spliter(dataset):
    """
        Split the dataset into 3 parts train, test and validation, create dataloader for each part
        
        dataset: (torch.data.Dataset class) Dataset on which split and dataloader will be created
        
        return: (torch.data.Dataloader) 3 Dataloader of train, test and validation
    """
    train_split, validataion_split = math.floor(dataset_parameter['dataset_split'][0]*len(dataset)), math.floor(dataset_parameter['dataset_split'][1]*len(dataset))
    test_split = len(dataset)-train_split-validataion_split
    train_set, val_set, test_set = random_split(dataset, (train_split, validataion_split, test_split))
    
    train_set = dataloader_creator(train_set)
    val_set = dataloader_creator(val_set)
    test_set = dataloader_creator(test_set)
    
    return train_set, val_set, test_set


def dataloader_spliter(dataset):
    """
        Split the dataset into 3 parts train, test and validation, create dataloader for each part
        
        dataset: (torch.data.Dataset class) Dataset on which split and dataloader will be created
        
        return: (torch.data.Dataloader) 3 Dataloader of train, test and validation
    """
    train_split, validataion_split = math.floor(dataset_parameter['dataset_split'][0]*len(dataset)), math.floor(dataset_parameter['dataset_split'][1]*len(dataset))
    test_split = len(dataset)-train_split-validataion_split
    train_set, val_set, test_set = random_split(dataset, (train_split, validataion_split, test_split))
    
    train_set = dataloader_creator(train_set)
    val_set = dataloader_creator(val_set)
    test_set = dataloader_creator(test_set)
    
    return train_set, val_set, test_set




"""
# Model architecture """

class MODEL(nn.Module):
    def __init__(self, model):
        """
        Initialize model and define last layers of fine-tuned model
        bert: BertModel from Huggingface
        return: None
        """
        super(MODEL, self).__init__()
        self.bert_model = model
        
        # word level with or without demo weights initialization
        if bert_model_parameter['word_level_mean_way'] == 1:
            self.dense = nn.Linear(768, training_parameter['hidden_layers'][0], bias=False)

        if model_parameter['word_level_mean_way'] == 2:
            self.register_parameter(name='model_adjacency_weight',
                                     param=torch.nn.Parameter(torch.rand(model_parameter['second_last_layer_size'],
                                                                       requires_grad=True)))
        elif model_parameter['word_level_mean_way'] == 3:
            self.register_parameter(name='word_level_emb_horizontal_weights',
                                     param=torch.nn.Parameter(torch.rand(dataset_parameter['max_seq_len'],
                                                                       requires_grad=True)))
        elif model_parameter['word_level_mean_way'] == 4:
            self.register_parameter(name='word_level_emb_batch_weights',
                                     param=torch.nn.Parameter(torch.rand(dataset_parameter['max_seq_len'], model_parameter['second_last_layer_size'],
                                                                       requires_grad=True)))
        elif model_parameter['word_level_mean_way'] == 5:
            self.flat_dense = nn.Linear(dataset_parameter['max_seq_len']*model_parameter['second_last_layer_size'], model_parameter['second_last_layer_size'])
            
        self.activation = training_parameter["activation_function"]
        self.relu1 = nn.ReLU()

        self.fc1 = nn.Linear(model_parameter['second_last_layer_size'], 128)
        self.dropout1 = nn.Dropout(args.drop_out)
        self.relu2 = nn.ReLU()
        self.fc2 = nn.Linear(128, 2)
        self.log_softmax = nn.LogSoftmax(dim=1)

        if args.encoder_frozen == 'True':
            for param in self.bert_model.parameters():
                param.requires_grad = False

    def forward(self, input_ids, attention_mask, word_level, word_level_len):
        """
        Pass input data from all layer of model

        input_id                  :   (list) Encoding vectors (INT) from BertTokenizerFast
        attention_mask            :   (list) Mask vector (INT [0,1]) from Bert BertTokenizerFast
        average_adjacency_matrix  :   (list of list) Average adj matrix (float (0:1)) defined with stanza dependancy graph and its degree multiplication
        word_level                :   (list) Contain number of sub words broken from parent word (INT)
        word_level_len            :   (INT) Define the length of parent sentence without any tokenization

        return: (float [0,1]) Last output from fine tuned model
        """

        # token level embeddings
        model_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)
        token_level_embedding = model_output.last_hidden_state

        # word level embeddings initialized
        word_level_embedding = torch.zeros(input_ids.shape[0], dataset_parameter['max_seq_len'], bert_model_parameter['second_last_layer_size'])

        # iterate all text in one batch
        for batch_no in range(0, input_ids.shape[0]):
            start, end = 0, 0
            for word_break_counter in range(0, word_level_len[batch_no]):
                start = end
                end = start + word_level[batch_no][word_break_counter]
                word_level_embedding[batch_no][word_break_counter] = torch.mean(token_level_embedding[batch_no][start:end], 0, True)
        word_level_embedding = word_level_embedding.to(device)

        if bert_model_parameter['word_level_mean_way']==1:
            word_level_embedding_flat = torch.flatten(word_level_embedding, start_dim=1)
            output = torch.mean(word_level_embedding, 1)
        elif bert_model_parameter['word_level_mean_way']==2:
            word_level_embedding_flat = torch.flatten(word_level_embedding, start_dim=1)
            word_level_embedding_mean = word_level_embedding * self.word_level_emb_vertical_weights
            output = torch.mean(word_level_embedding_mean, 1)
        elif bert_model_parameter['word_level_mean_way']==3:
            word_level_embedding_flat = torch.flatten(word_level_embedding, start_dim=1)
            word_level_embedding_mean =  word_level_embedding.permute(0,2,1) * self.word_level_emb_horizontal_weights
            output = torch.mean(word_level_embedding_mean.permute(0,2,1), 1)
        elif bert_model_parameter['word_level_mean_way']==4:
            word_level_embedding_flat = torch.flatten(word_level_embedding, start_dim=1)
            word_level_embedding_mean = word_level_embedding * self.word_level_emb_batch_weights
            output = torch.mean(word_level_embedding_mean, 1)
        elif bert_model_parameter['word_level_mean_way']==5:
            word_level_embedding_flat = torch.flatten(word_level_embedding, start_dim=1)
            output = model.flat_dense(word_level_embedding_flat)
        first_fc_layer_emb = output #its size will be 768 in any mean-type way

        x = self.relu1(output)
        x = self.fc1(output)
        x = self.relu2(x)
        x = self.dropout1(x)
        x = self.fc2(x)    
        x = self.log_softmax(x)
        return x, first_fc_layer_emb, word_level_embedding_flat
        # return output, dense_layer_emb,  word_level_embedding_flat #also returning word_level_embedding to use it in calculating relevance at word-level

    def predict(self, text):
        # processed_text = preprocess_text(text)
  


        # input_ids, attention_mask, word_break, word_break_len = tokenize_word_ritwik([processed_text])
        input_ids, attention_mask, word_break, word_break_len = tokenize_word_ritwik2([text])

        input_ids, attention_mask, word_break, word_break_len = torch.tensor(input_ids), torch.tensor(attention_mask),  torch.tensor(word_break), torch.tensor(word_break_len)


        # put all feature variable on device
        input_ids, attention_mask, word_break, word_break_len = input_ids.to(device), attention_mask.to(device),  word_break.to(device), word_break_len.to(device)

        # get prediction from model
        with torch.no_grad():
            logits, first_fc_layer_emb, word_level_embedding_flat = self.forward(input_ids, attention_mask, word_break, word_break_len)


        label_proba = np.exp(logits.detach().cpu().numpy()[0])
        return label_proba, first_fc_layer_emb, word_level_embedding_flat

model = MODEL(bert)
model = model.to(device)



"""# Training and evaluating functions"""

def train(epoch):
    """
        In this model will be trained on train_dataloader which is defined earlier

        epoch: epoch for verbose

        return: loss and accuracy on whole training data (mean taken on batch)
    """
    running_batch_loss = 0.0
    running_batch_accuracy = 0.0
    
    model.train()
    with tqdm(train_dataloader, unit="batch") as tepoch:
      
        for input_ids, attention_mask,  word_level, word_level_len, label in tepoch:
            tepoch.set_description(f"Training Epoch {epoch}")
            
            input_ids, attention_mask,  word_level, word_level_len, label = input_ids.to(device), attention_mask.to(device),  word_level.to(device), word_level_len.to(device), label.to(device)
             
            optimizer.zero_grad()
            
            preds, first_fc_layer_emb, word_level_embedding_flat = model(input_ids, attention_mask,  word_level, word_level_len)
            # print('pred: ', preds)
            # print('label: ', label)

            loss = loss_fn(preds, label)

            loss.backward()
            optimizer.step()
            
            batch_loss = loss.item()
            running_batch_loss += batch_loss
            
            batch_accuracy = (((preds.max(1)[1]==label).sum()).item())/label.size(0)
            running_batch_accuracy += batch_accuracy
            
            tepoch.set_postfix(loss=batch_loss, accuracy=(batch_accuracy))

            del input_ids, attention_mask,  word_level, word_level_len, label, preds                 
            
    return running_batch_loss/len(train_dataloader), running_batch_accuracy/len(train_dataloader)
       
def validate(epoch):
    """
        In this model will be validation without changing any parameter of model on val_dataloader

        epoch: epoch for verbose

        return: loss and accuracy on whole training data (mean taken on batch)
    """

    running_batch_loss = 0.0
    running_batch_accuracy = 0.0
    
    model.eval()
    with tqdm(val_dataloader, unit="batch") as tepoch:
        
        for input_ids, attention_mask, word_level, word_level_len, label in tepoch:
            tepoch.set_description(f"Validation Epoch {epoch}")
            
            input_ids, attention_mask,  word_level, word_level_len, label = input_ids.to(device), attention_mask.to(device),  word_level.to(device), word_level_len.to(device), label.to(device)
            
            with torch.no_grad():   
                preds, first_fc_layer_emb, word_level_embedding_flat = model(input_ids, attention_mask, word_level, word_level_len)
                loss = loss_fn(preds, label)
            
                batch_loss = loss.item()
                running_batch_loss += batch_loss
            
                batch_accuracy = (((preds.max(1)[1]==label).sum()).item())/label.size(0)
                running_batch_accuracy += batch_accuracy
                
            tepoch.set_postfix(loss=batch_loss, accuracy=(batch_accuracy))

            del input_ids, attention_mask,  word_level, word_level_len, label, preds         
            
    return running_batch_loss/len(test_dataloader), running_batch_accuracy/len(test_dataloader)

def test():
    """
        Test model on test dataset

        return: loss and accuracy on whole training data (mean taken on batch)
    """
    running_batch_loss = 0.0
    running_batch_accuracy = 0.0

    predictions = []
    true_labels = []
    
    model.eval()
    with tqdm(test_dataloader, unit="batch") as tepoch:
        
        for input_ids, attention_mask, word_level, word_level_len, label in tepoch:
            tepoch.set_description(f"Testing on test dataset")
            
            input_ids, attention_mask, word_level, word_level_len, label = input_ids.to(device), attention_mask.to(device),  word_level.to(device), word_level_len.to(device), label.to(device)
            
            with torch.no_grad():
                preds, first_fc_layer_emb, word_level_embedding_flat = model(input_ids, attention_mask,  word_level, word_level_len)
                loss = loss_fn(preds, label)

                batch_loss = loss.item()
                running_batch_loss += batch_loss
            
                batch_accuracy = (((preds.max(1)[1]==label).sum()).item())/label.size(0)
                running_batch_accuracy += batch_accuracy

                preds = preds.detach().cpu().numpy()
                predictions.append(preds)

                label = label.detach().cpu().numpy()
                true_labels.append(label)

            del input_ids, attention_mask, word_level, word_level_len, label, preds
            
            tepoch.set_postfix(loss=batch_loss, accuracy=(batch_accuracy))
    
    predictions = np.concatenate(predictions, axis=0)    
    predictions = np.argmax(predictions, axis = 1)

    true_labels = np.concatenate(true_labels, axis=0) 

    return running_batch_loss/len(val_dataloader), running_batch_accuracy/len(val_dataloader), predictions, true_labels

"""# Train model"""

# optimizer for model
optimizer = AdamW(model.parameters(), lr = training_parameter['optimizer_parameter']['lr'])

# as we are using NLL loss function we can use class weights for imbalance dataset
weights = torch.tensor(training_parameter['loss_func_parameter']['class_weight'], dtype=torch.float)
weights = weights.to(device)

# initialize loss
loss_fn  = nn.NLLLoss(weight=weights)




"""## LRP"""

def my_softmax(x):
    """Compute softmax values for each sets of scores in x."""
    return np.array(torch.softmax(torch.tensor(x), 0))
#following is the most recent version of LRP fn
def LRP(sentence, model, lrp_variant, used_bias, concerned_layers):
    ''' This function calculates the relevance value of each word corresponding to model's predicted output for given input sentence.
      params: sentence: input sentence
              model: trained classifier model
              lrp_varaint: (int)
              lrp_variant:  0 --> 0 LRP (for output layer)
                            1 --> gamma LRP (for lower layers)
                            2 --> epsilon LRP (for upper layers)
               used_bias: boolean, (used bias or not in fc layers of classifier)


      returns: a tuple of (inputsentence, predicted_value p, input_words_relevances value of each word in input sentence) '''
    # print('label: ', end=' ')
    model.eval()
    output_proba, first_layer_emb, word_level_emb = model.predict(sentence)
    # print('output_proba: ', output_proba)


    # if lrp_variant==0:
    #   print('==================================0 LRP======================================================================')
    # elif lrp_variant==1:
    #   print('==================================GAMMA-LRP======================================================================')
    # elif lrp_variant==2:
    #   print('==================================EPSILON-LRP======================================================================')

      

    #Step 1: ----------------Extract layers---------------------------------------------------------------------
    layers = list(model._modules)[1:]
    # print("All layers: ", layers)
    layers = [layer for layer in layers if layer in concerned_layers]
    n_layers = len(layers)
    # print("filtered layers: ", layers)

    if bert_model_parameter['word_level_mean_way']!=5:
      first_layer_emb = first_layer_emb.cpu().detach().numpy() #that 768-size vector after taking mean according to given mean-way
      first_layer_emb = first_layer_emb.reshape(1, -1)   #reshapping for matrix multiplication
      first_activation =  first_layer_emb
    else:
      first_activation = word_level_emb.cpu().detach().numpy()


    #initializing activation of input layer with input text_emb

    activations = [first_activation] + [None]*n_layers


    #step2: Propagate input text_emb through each layer and store activations------------------------------------
    for layer in range(n_layers):
      w = model._modules[layers[layer]].state_dict()['weight'].cpu().detach().numpy()

      if used_bias==True:        #if used biases in fc layers of text classifier then next layer activation = w.x+b
        b = model._modules[layers[layer]].state_dict()['bias'].cpu().detach().numpy()

        if layer==n_layers-1:    #if last layer then apply log_softmax() activation fn on this layer
          # activations[layer+1] = np.log(np.array([my_softmax(activations[layer].dot(w.T+b)[0])])) #log_softmax(w.x+b)
          activations[layer+1] = np.array([my_softmax(activations[layer].dot(w.T+b)[0])]) #only softmax(w.x+b) 
    
        else:                    #if other layers then apply rely activation fn
          activations[layer+1] = np.maximum(0, activations[layer].dot(w.T)+b) #relu(w.x) 

      else:                     #if didn't used biases in fc layers of text classifier then next layer activation = w.x only
        if layer==n_layers-1:
            # activations[layer+1] = np.log(np.array([my_softmax(activations[layer].dot(w.T)[0])])) #log_softmax(w.x+b) 
          activations[layer+1] = np.array([my_softmax(activations[layer].dot(w.T)[0])]) #only softmax(w.x+b) 
        else:
          activations[layer+1] = np.maximum(0, activations[layer].dot(w.T)) #relu(w.x)
     

    output_activation = activations[-1]
    output_proba = output_activation
 
    #----------------------NOT DOING STEP-3---------------------
    max_activation = output_activation.max()        
    #step3:---In output layer, except the true class activation, set all other class's values to 0----------------
    # output_activation = [val if val == max_activation else 0 for val in output_activation]

    activations[-1] = output_activation
    activations[-1]  = np.array(activations[-1]).reshape(1, dataset_parameter['num_classes'])   #reshaped it to 1*num_classes


      
    #step4: -------Backpropagate relevance values from output layer to input layer--------------------------------------

    #initialize output layer's relevance to its activation, rest layer's as None
    relevances = [None]*n_layers + [activations[-1]]


    # print('activations[-1]: ', activations[-1])

    # Iterate over the layers in reverse order
    for layer in range(n_layers)[::-1]:
      #getting weights of the layer
      w = model._modules[layers[layer]].state_dict()['weight'].T.cpu().detach().numpy()
    # b = model._modules[layers[layer]].state_dict()['bias'].cpu().detach().numpy()
    
    # rho is a function to transform the weights
      if lrp_variant==0:      #LRP-0 RULE
        rho = lambda p: p     
        incr = lambda z: z+1e-9 
      elif lrp_variant==1:    #LRP-gamma RULE
        gamma = 0.1
        rho = lambda p: p + gamma*p.clip(min=0)

        incr = lambda z: z+1e-9
      elif lrp_variant==2:    #LRP-epsilon RULE
        epsilon = 0.1
        rho = lambda p: p
        incr = lambda z: z+1e-9 + epsilon*((z**2).mean()**.5)
        
      w = rho(w)  
      # b = rho(b)                              #bias ignored 'cause we want relevane to only flow to the input neuron and not end up in static bias neurons
        
      z = incr(activations[layer].dot(w))       # step 1 : this step determines the sum of influences for each neuron in the higher layer, analogous to a modified forward pass
      s = relevances[layer+1]/z                 # step 2 : element-wise division as per formula in the paper
      c = s.dot(w.T)                            # step 3

      
      relevances[layer] = activations[layer]*c  # step 4 : element-wise product and store the value as relevance of current layer


    #distribut4 768-size relevance of 1st layer to word-level relevance if dim 50*768 according to "bert_model_parameter['word_level_mean_way']"
    bert_embedding_level_relevances = np.zeros((dataset_parameter['max_seq_len'], bert_model_parameter['second_last_layer_size']))
    bert_embeddings = word_level_emb.reshape(dataset_parameter['max_seq_len'], bert_model_parameter['second_last_layer_size']).to('cpu')  #50*768

    if(bert_model_parameter['word_level_mean_way']==1):                     #way-1 (simple mean)
      for col in range(bert_model_parameter['second_last_layer_size']):     #simple vertical mean of each column
        bert_embedding_level_relevances[:, col] = (bert_embeddings[:, col]/bert_embeddings[:, col].sum())*relevances[0][0][col]

    elif(bert_model_parameter['word_level_mean_way']==2):                   #way-2 (weighted vertical mean (vertical weights))
      for row in range(dataset_parameter['max_seq_len']):                   
        word_level_emb_vertical_weights=[]
        with torch.no_grad():
          word_level_emb_vertical_weights = model.word_level_emb_vertical_weights.cpu().detach().numpy()

        bert_embeddings[row] = bert_embeddings[row]*word_level_emb_vertical_weights
      
      for col in range(bert_model_parameter['second_last_layer_size']):
        bert_embedding_level_relevances[:, col] = (bert_embeddings[:, col]/bert_embeddings[:, col].sum())*relevances[0][0][col]

    elif(bert_model_parameter['word_level_mean_way']==3):                   #way-3, weighted vertical mean (horizontal weights)
      word_level_emb_horizontal_weights = model.word_level_emb_horizontal_weights.cpu().detach().numpy()
      # print('word_level_emb_horizontal_weights finally: ', word_level_emb_horizontal_weights)

      for col in range(bert_model_parameter['second_last_layer_size']):
        bert_embeddings[:, col] = bert_embeddings[:, col]*word_level_emb_horizontal_weights

      for col in range(bert_model_parameter['second_last_layer_size']):
        bert_embedding_level_relevances[:, col] = (bert_embeddings[:, col]/bert_embeddings[:, col].sum())*relevances[0][0][col]

    elif(bert_model_parameter['word_level_mean_way']==4):                   #way-4, weightd vertical mean (weight matrix)
      word_level_emb_batch_weights = model.word_level_emb_batch_weights.detach().numpy()
      bert_embeddings = bert_embeddings*word_level_emb_batch_weights
      for col in range(bert_model_parameter['second_last_layer_size']):
        bert_embedding_level_relevances[:, col] = (bert_embeddings[:, col]/bert_embeddings[:, col].sum())*relevances[0][0][col]

    elif(bert_model_parameter['word_level_mean_way']==5):
      bert_embedding_level_relevances = relevances[0][0].reshape(dataset_parameter['max_seq_len'], bert_model_parameter['second_last_layer_size'])


    input_words_relevances = []
    for word in range(dataset_parameter['max_seq_len']):
      input_words_relevances.append(sum(bert_embedding_level_relevances[word]))

    p = torch.sigmoid(torch.tensor(max_activation)).item()                          #predicted output for the given input  

    #taking softmax of relevance values
    # input_words_relevance = [relevance.item() for relevance in torch.softmax(torch.tensor(word_relevances), dim=0)]
    # input_words_relevance = [val.item() for val in torch.softmax(torch.tensor(input_words_relevance), dim=0)]      
    return (sentence, p, input_words_relevances, output_proba)
      # return (p, input_words_relevance)

import matplotlib
import seaborn as sns
import colorsys

def visualize_word_level_relevance(sentence, word_relevances , only_sentence_word_lrp):
    '''This function visualizes the relevance of each word in input sentence
     params: 
              input sentence: string 
              word_relevances (returned by LRP() function) : array of wo relevances
              only_sentence_word_lrp: boolean, if it is True then show relevance of words present in sentence else relevance of each token upto max_length including [CLS], [PAD] token's relevance
      returns:
           df.style.background_gradient(axis=1, gmap=df.iloc[0], cmap='inferno')
    '''
    relevances = word_relevances

    sentence = re.sub(r'\s+', ' ', sentence)
    tokens = sentence.split(' ')    
    if only_sentence_word_lrp:
      relevances = relevances[:len(tokens)]



      df = pd.DataFrame(relevances).T
      df.columns = ['w'+str(i)+"_"+tokens[i] for i in range(len(tokens))]
      return df.style.background_gradient(axis=1, gmap=df.iloc[0], cmap='inferno')
    else:
      #get relevance value of padding tokens as well
      while(len(tokens)!=dataset_parameter['max_seq_len']):
        tokens.append('PAD')
      relevances = relevances[:len(tokens)]


      df = pd.DataFrame(relevances).T
      df.columns = ['w'+str(i)+"_"+tokens[i] for i in range(len(tokens))]
      return df.style.background_gradient(axis=1, gmap=df.iloc[0], cmap='inferno')

def visualize_phrase_level_relevance(sentence, word_relevances):
    '''This function visualizes the relevance of each phrase in input sentence after doing phrase level splitting
     params: 
              input sentence: string 
              word_relevances (returned by LRP() function) : array of wo relevances
              only_sentence_word_lrp: boolean
      returns:
           df.style.background_gradient(axis=1, gmap=df.iloc[0], cmap='inferno')
    '''
    relevances = word_relevances
    sentence = re.sub(r'\s+', ' ', sentence)
    tokens = sentence.split(' ')
    relevances = word_relevances[:len(tokens)]
    temp_sentence = '\t'.join(tokens)

    #finding phrases in the input sentences and simultaneously calculating phrase-level relevances
    ctags = predict_with_model(chunk_model, temp_sentence, chunk_tokenizer)  #this function is imported from chunking_model.py


    words_arr = temp_sentence.split('\t')
    phrase_tokens=[]
    phrase_relevances = []

    prev_token= words_arr[0]
    prev_relevance = relevances[0]
    for i in range(1, len(ctags)):

      if ctags[i][0]=='B':
        phrase_tokens.append(prev_token)
        phrase_relevances.append(prev_relevance)

        prev_token=words_arr[i]
        prev_relevance = relevances[i]
      elif ctags[i][0]=='I':
        prev_token= prev_token+'\t'+words_arr[i]
        prev_relevance+=relevances[i]
    phrase_tokens.append(prev_token)
    phrase_relevances.append(prev_relevance)

    #phrase_relevances = my_softmax(phrase_relevances) #taking softmax
    relevance_sum = sum(phrase_relevances)
    relevances_normalized = [r/relevance_sum for r in phrase_relevances]




    df = pd.DataFrame(relevances_normalized).T
    df.columns = ['phrase'+str(i)+"_"+phrase_tokens[i] for i in range(len(phrase_tokens))]
    return df.style.background_gradient(axis=1, gmap=df.iloc[0], cmap='inferno')







def lime_predict_proba(text_arr):
    token_ids = tokenizer(
                                text_arr,
                                max_length=dataset_parameter['max_seq_len']+2,
                                padding=True,
                                truncation=True,
                                return_token_type_ids=False
                                )      
  # convert and process all data to torch tensor
    input_ids = torch.tensor(token_ids['input_ids'])
    attention_mask = torch.tensor(token_ids['attention_mask'])

  #calculate word break list from functions

    word_break = list(map(word_break_list, text_arr))
    word_break_len = torch.tensor(list(map(len, word_break))) 
    word_break = torch.tensor([sent + [0]*(dataset_parameter['max_seq_len'] - len(sent)) for sent in word_break])


  #put all feature variable on device
    input_ids, attention_mask, word_break, word_break_len = input_ids.to(device), attention_mask.to(device),  word_break.to(device), word_break_len.to(device)

  #get prediction from model
    with torch.no_grad():
        pred_proba, _, __ = model(input_ids, attention_mask, word_break, word_break_len)

    return pred_proba.cpu().detach().numpy()





import numpy as np
import lime
import torch
import torch.nn.functional as F
from lime.lime_text import LimeTextExplainer

from transformers import AutoTokenizer, AutoModelForSequenceClassification

class_names = ['non-toxic','toxic']


explainer = LimeTextExplainer(class_names=class_names)



import sys

# Read the arguments
# lrp_or_lime = sys.argv[1]



# if lrp_or_lime=='LRP':

"========================LRP EXPLANATIONS IN ERASER FORMAT=========================="

if not os.path.exists('explanation_dicts'):
    os.makedirs('explanation_dicts')


savepath = args.model_path


checkpoint = torch.load(savepath, map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['model_state_dict']) 


model_name = savepath.split('.')[0].split('/')[-1]

print('model_name: ', model_name)

#test file
test_file = args.data_path + 'test_split'+str(args.split)+'.jsonl'


hateXplain_df = pd.read_json(args.data_path + 'dataset.json')
hateXplain_df = hateXplain_df.T
# LRP_VARIANT = 0



# for THRESHOLD in thresholds:

if args.method!='lime':
    for i in range(0, 1):
        LRP_VARIANT = int(args.method)
        THRESHOLD =0.50
        print('LRP_VARIANT: ', LRP_VARIANT)
        print()
        with open(test_file) as f:
          test_data = [json.loads(line) for line in f]
        test_docid =[]
        test_texts =[]

        for i in tqdm(range(len(test_data)), desc='getting docids and corresponding text...'):
          test_docid.append(test_data[i]['annotation_id'])
          test_texts.append(" ".join(list(hateXplain_df[hateXplain_df.post_id==test_data[i]['annotation_id']]['post_tokens'])[0]))


        test_df = pd.DataFrame(test_docid, columns=['annotation_id'])
        test_df['commentText'] = test_texts
        test_df.head()



        #classification and classification_scores
        def run_lrp(test_df):
            all_labels  = []
            all_output_proba  = []
            all_LRP_scores = []

            for i in tqdm(range(test_df.shape[0]), desc='getting LRP scores...'):
                text = test_df.iloc[i]['commentText']

                concerned_layers = ['flat_dense', 'fc1', 'fc2']
                sentence_text, p, word_relevances, output_proba = LRP(text, model, LRP_VARIANT, True, concerned_layers) #relevances with Bias=True


                non_toxic_proba = output_proba[0][0] #since output_proba is a 2*1 array
                toxic_proba = output_proba[0][1]

                label = 'non-toxic'
                if toxic_proba>non_toxic_proba:
                  label = 'toxic'

                n = len(sentence_text.split(' '))
                word_relevances = word_relevances[0: n]
                min_relevance = min(word_relevances)
                if min_relevance<0:
                  word_relevances = [relevance-min_relevance for relevance in word_relevances] #adding +ve of most -ve value to every relevance score
                  word_relevances = word_relevances/max(word_relevances)
                else:
                  word_relevances = word_relevances/max(word_relevances)

                # print('word_relevances :', word_relevances)
                all_labels.append(label)
                all_output_proba.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})
                all_LRP_scores.append(word_relevances)
            return all_labels, all_output_proba, all_LRP_scores




        all_labels, all_output_proba, all_LRP_scores =  run_lrp(test_df)

        test_df['classification'] = all_labels
        test_df['classification_scores'] = all_output_proba
        test_df['LRP_scores'] = all_LRP_scores




        #creating rationales

        def find_ranges(L):
            if len(L)!=0:
                first = last = L[0]
                for n in L[1:]:
                    if n - 1 == last: # Part of the group, bump the end
                        last = n
                    else: # Not part of the group, yield current group and start a new
                        yield first, last
                        first = last = n
                yield first, last # Yield the last group


        def get_rationales(post_id, LRP_scores):
            soft_rationale_predictions = LRP_scores
            curr_thres= THRESHOLD
            flag = False    #appropriate threshold found or not
            hard_rationale = []
            while flag==False:
                ones_count = 0
                for idx in range(len(LRP_scores)):
                    if LRP_scores[idx]>curr_thres:
                        ones_count+=1
                        hard_rationale.append(1)
                    else:
                        hard_rationale.append(0)
                if ones_count==len(LRP_scores): #if all tokensa are relevant then increase the threshold by 0.1
                    curr_thres+=0.1
                    hard_rationale = []
                else:
                    flag = True    #appropriate threshold found!


            rationales = []
            indexes = sorted([i for i, each in enumerate(hard_rationale) if each==1])
            span_list = list(find_ranges(indexes))

            hard_rationale_predictions = []
            for each in span_list:
                if type(each)== int:
                    start = each
                    end = each+1
                elif len(each) == 2:
                    start = each[0]
                    end = each[1]+1
                else:
                    print('error')

                hard_rationale_predictions.append({
                  "start_token": start,
                  "end_token": end})

            rationales.append({
            'docid' : post_id,
            'hard_rationale_predictions': hard_rationale_predictions,
            'soft_rationale_predictions': soft_rationale_predictions,
            'threshold': curr_thres

          })
            return rationales


        rationales_for_output_file = []


        for i in range(test_df.shape[0]):
            post_id = test_df.iloc[i]['annotation_id']

            LRP_scores = list(test_df.iloc[i]['LRP_scores'])
            rationales_for_output_file.append(get_rationales(post_id, LRP_scores))

        test_df['rationales'] = rationales_for_output_file




        # def get_topK_indices( soft_rationale_predictions, k=5):
        # 	isTopk = args.faithfullness_filtering=='top-k'


        # 	if isTopk:
        # 		index = list(range(len(soft_rationale_predictions)))
        # 		s = sorted(index, reverse=True, key=lambda i: soft_rationale_predictions[i])
        # 		return s[:k]
        # 	#else removing all tokens have LRP value>threshold
        # 	threshold = float(args.faithfullness_filtering)
        # 	indices_to_remove = [idx for idx, x in enumerate(soft_rationale_predictions) if x>threshold] #find the indexes of all the relevent tokens

        # 	assert len(indices_to_remove)!=len(soft_rationale_predictions), 'Inside get_topK_indices(), all tokens removed'
        # 	return indices_to_remove


        # def get_topK_indices(threshold, soft_rationale_predictions, k=5):
        #   index = range(len(soft_rationale_predictions))
        #   s = sorted(index, reverse=True, key=lambda i: soft_rationale_predictions[i])
        #   return s[:k]
        # #removing all tokens have LRP value>0.5 as of now! (can consider top K tokens as well)
        #   indices_to_remove = [idx for idx, x in enumerate(soft_rationale_predictions) if x>threshold] #find the indexes of all the relevent tokens

        #   assert len(indices_to_remove)!=len(soft_rationale_predictions), 'Inside get_topK_indices(), all tokens removed'
        #   return indices_to_remove




        # test_df['to_remove_indices'] = test_df['rationales'].apply(lambda r : get_topK_indices(r[0]['threshold'], r[0]['soft_rationale_predictions']))
        # test_df.head()

        def get_topK_indices( soft_rationale_predictions, k=5):
            isTopk = args.faithfullness_filtering=='top-k'


            if isTopk:
                index = list(range(len(soft_rationale_predictions)))
                s = sorted(index, reverse=True, key=lambda i: soft_rationale_predictions[i])
                return s[:k]
            #else removing all tokens have LRP value>threshold
            threshold = float(args.faithfullness_filtering)
            indices_to_remove = [idx for idx, x in enumerate(soft_rationale_predictions) if x>threshold] #find the indexes of all the relevent tokens

            assert len(indices_to_remove)!=len(soft_rationale_predictions), 'Inside get_topK_indices(), all tokens removed'
            return indices_to_remove


        # test_df['to_remove_indices'] = test_df['rationales'].apply(lambda r : get_topK_indices(r[0]['soft_rationale_predictions']))


        test_df['to_remove_indices'] = test_df['rationales'].apply(lambda r : get_topK_indices(r[0]['soft_rationale_predictions']))
        
        # def get_topK_indices(threshold, soft_rationale_predictions, k=None):
        #   # index = range(len(lst))
        #   # s = sorted(index, reverse=True, key=lambda i: lst[i])
        #   # return s[:k]
        # #removing all tokens have LRP value>0.5 as of now! (can consider top K tokens as well)
        # 	indices_to_remove = [idx for idx, x in enumerate(soft_rationale_predictions) if x>threshold] #find the indexes of all the relevent tokens

        # 	assert len(indices_to_remove)!=len(soft_rationale_predictions), 'Inside get_topK_indices(), all tokens removed'
        # 	return indices_to_remove

        # test_df['to_remove_indices'] = test_df['rationales'].apply(lambda r : get_topK_indices(r[0]['threshold'], r[0]['soft_rationale_predictions']))


        def remove_topK_relevent_words(post_tokens, to_remove_indices):
            new_text = [v for i, v in enumerate(post_tokens) if i not in to_remove_indices]
            return " ".join(new_text)


        def get_topK_relevent_words(post_tokens, to_remove_indices):
            new_text = [post_tokens[i] for i in to_remove_indices]
            return " ".join(new_text)





        texts_after_removing = []
        for i in range(test_df.shape[0]):
            token_list = test_df.iloc[i]['commentText'].split(' ')
            to_remove_indices = test_df.iloc[i]['to_remove_indices']
            texts_after_removing.append(remove_topK_relevent_words(token_list, to_remove_indices))

        text_of_top_relevant_tokens = []
        for i in range(test_df.shape[0]):
            token_list = test_df.iloc[i]['commentText'].split(' ')
            to_remove_indices = test_df.iloc[i]['to_remove_indices']
            text_of_top_relevant_tokens.append(get_topK_relevent_words(token_list, to_remove_indices))

        test_df['text_after_removing_relevant_tokens'] = texts_after_removing
        test_df['text_of_top_relevant_tokens'] = text_of_top_relevant_tokens


        def run_model_after_removing_top_relevant_tokens(test_df):
            comprehensiveness_classification_scores  = []

            for i in tqdm(range(test_df.shape[0]), desc='getting comprehensiveness scores...'):
                text = test_df.iloc[i]['text_after_removing_relevant_tokens']
                label_proba,_, _ = model.predict(text)
                non_toxic_proba = label_proba[0] 
                toxic_proba = label_proba[1]
                label = 'non-toxic'
                if toxic_proba>non_toxic_proba:
                    label = 'toxic'
                comprehensiveness_classification_scores.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})
            return comprehensiveness_classification_scores

        test_df['comprehensiveness_classification_scores'] = run_model_after_removing_top_relevant_tokens(test_df)

        #suffiency score
        def run_model_on_top_relevant_tokens(test_df):

            sufficiency_classification_scores  = []

            for i in tqdm(range(test_df.shape[0]), desc='getting sufficiency scores...'):
                text = test_df.iloc[i]['text_of_top_relevant_tokens']


                label_proba,_, _ = model.predict(text)
                non_toxic_proba = label_proba[0] #since output_proba is a 2*1 array
                toxic_proba = label_proba[1]

                label = 'non-toxic'
                if toxic_proba>non_toxic_proba:
                  label = 'toxic'


                sufficiency_classification_scores.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})

            return sufficiency_classification_scores

        test_df['sufficiency_classification_scores'] = run_model_on_top_relevant_tokens(test_df)
        test_df.head()# test_df.to_json('original_test_df.json', orient="records")



        #thresholded_scores calculation
        thresholded_scores = []
        for i in range(test_df.shape[0]):
            temp_list = []
            temp_dict = {}
            temp_dict['threshold'] = 0.5
            temp_dict['comprehensiveness_classification_scores'] = test_df.iloc[i]['comprehensiveness_classification_scores']
            temp_dict['sufficiency_classification_scores'] = test_df.iloc[i]['sufficiency_classification_scores']

            temp_list.append(temp_dict)

            thresholded_scores.append(temp_list)

        test_df['thresholded_scores'] = thresholded_scores


        test_df = test_df[['annotation_id', 'rationales',  'classification', 'classification_scores', 'comprehensiveness_classification_scores', 'sufficiency_classification_scores', 'thresholded_scores']]


        # Set the path of the directory and create it if it doesn't exist
        dir_path = 'explanation_dicts'
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

        # Set the path of the output file
        output_file_path = os.path.join(dir_path, model_name + '-LIME-' + 'faithfullness_filtering-' + args.faithfullness_filtering + '.json')

        # Print the output file path to check if it's what you expect
        print("Output file path:", output_file_path)

        # Write the JSON data to the output file
        test_df.to_json(output_file_path, orient="records")

        # Print a message to confirm that the file was saved
        if os.path.isfile(output_file_path):
            print("================File saved successfully=========")
        else:
            print(" ===============File not saved=================")



# 	"========================LIME EXPLANATIONS IN ERASER FORMAT=========================="

else:
    hateXplain_df = pd.read_json(args.data_path+'dataset.json')
    hateXplain_df = hateXplain_df.T
    thresholds = [0.50, 0.55, 0.60, 0.65]
    THRESHOLD = 0.50


    with open(test_file) as f:
        test_data = [json.loads(line) for line in f]
    #classification and classification_scores

    test_docid =[]
    test_texts =[]

    for i in tqdm(range(len(test_data)), desc = 'getting texts and docids...'):
        test_docid.append(test_data[i]['annotation_id'])
        test_texts.append(" ".join(list(hateXplain_df[hateXplain_df.post_id==test_data[i]['annotation_id']]['post_tokens'])[0]))


    test_df = pd.DataFrame(test_docid, columns=['annotation_id'])
    test_df['commentText'] = test_texts
    test_df.head()


    omitted_tokens = []
    def run_lime(test_df):
        all_labels  = []
        all_output_proba  = []
        all_lime_scores = []
        all_post_tokens_cleaned = []

        for i in tqdm(range(test_df.shape[0]), desc='getting lime scores...'):
            text = test_df.iloc[i]['commentText']
            # text = preprocess_text(text)


            explanation = explainer.explain_instance(text, lime_predict_proba, num_features=50, num_samples=100)
            lime_score_dict = {}
            for ele in explanation.as_list():
                lime_score_dict[ele[0]] = ele[1]

            lime_score_list = []

            text_tokens = text.split(' ')
            text_tokens = [tk for tk in text_tokens if tk!='']

            for token in text_tokens:
                if token in lime_score_dict:
                    lime_score_list.append(lime_score_dict[token])
                else:
                    omitted_tokens.append((i, token))
                    lime_score_list.append(0) #add 0 relevance to omitted-token


            min_lime_score = min(lime_score_list)
            lime_score_list = [score-min_lime_score for score in lime_score_list]
            max_lime_score = max(lime_score_list)
            
            assert max_lime_score!=0, 'max_lime_score is 0'

            lime_score_list = [score/max_lime_score for score in lime_score_list]

            assert len(lime_score_list) == len(text_tokens), 'lime_score_list is not equal to text_tokens'

            output_proba = np.exp(explanation.predict_proba) #exp(of log_softmax output) --> probability distribution for the nodes
            non_toxic_proba = float(output_proba[0])
            toxic_proba = float(output_proba[1])

            label = 'non-toxic'
            if toxic_proba>non_toxic_proba:
                label = 'toxic'

      

            all_labels.append(label)
            all_output_proba.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})
            all_lime_scores.append(lime_score_list)
            all_post_tokens_cleaned.append(text_tokens)

        return all_labels, all_output_proba, all_lime_scores, all_post_tokens_cleaned




    all_labels, all_output_proba, all_lime_scores, all_post_tokens_cleaned =  run_lime(test_df)
    test_df['post_tokens_cleaned'] = all_post_tokens_cleaned
    test_df['classification'] = all_labels
    test_df['classification_scores'] = all_output_proba
    test_df['lime_scores'] = all_lime_scores



    #creating rationales

    def find_ranges(L):
      # L = [1 if ele>0.5 else 0 for ele in L]
        if len(L)!=0:
            first = last = L[0]
            for n in L[1:]:
                if n - 1 == last: # Part of the group, bump the end
                    last = n
                else: # Not part of the group, yield current group and start a new
                    yield first, last
                    first = last = n
            yield first, last # Yield the last group



    def get_rationales(post_id, lime_scores):
        soft_rationale_predictions = lime_scores


        curr_thres= THRESHOLD
        flag = False    #appropriate threshold found or not
        hard_rationale = []
        while flag==False:
            ones_count = 0
            for idx in range(len(lime_scores)):
                if lime_scores[idx]>curr_thres:
                    ones_count+=1
                    hard_rationale.append(1)
                else:
                    hard_rationale.append(0)
            if ones_count==len(lime_scores): #if all tokens are relevent then increase the threshold by 0.1
                curr_thres+=0.1
                hard_rationale = []
            else:
                flag = True    #appropriate threshold found!



        rationales = []
        indexes = sorted([i for i, each in enumerate(hard_rationale) if each==1])
        span_list = list(find_ranges(indexes))

        hard_rationale_predictions = []
        for each in span_list:
            if type(each)== int:
                start = each
                end = each+1
            elif len(each) == 2:
                start = each[0]
                end = each[1]+1
            else:
                print('error')

            hard_rationale_predictions.append({
                "start_token": start,
                "end_token": end})
          
        rationales.append({
          'docid' : post_id,
          'hard_rationale_predictions': hard_rationale_predictions,
          'soft_rationale_predictions': soft_rationale_predictions,
          'threshold': curr_thres

      })
        return rationales



    rationales_for_output_file = []


    for i in tqdm(range(test_df.shape[0]), desc='getting rationales...'):
        post_id = test_df.iloc[i]['annotation_id']

        lime_scores = list(test_df.iloc[i]['lime_scores'])
        rationales_for_output_file.append(get_rationales(post_id, lime_scores))

    test_df['rationales'] = rationales_for_output_file

    # #helper fns for sufficiency calculation
    def get_topK_indices( soft_rationale_predictions, k=5):
        isTopk = args.faithfullness_filtering=='top-k'


        if isTopk:
            index = list(range(len(soft_rationale_predictions)))
            s = sorted(index, reverse=True, key=lambda i: soft_rationale_predictions[i])
            return s[:k]
        #else removing all tokens have LRP value>threshold
        threshold = float(args.faithfullness_filtering)
        indices_to_remove = [idx for idx, x in enumerate(soft_rationale_predictions) if x>threshold] #find the indexes of all the relevent tokens

        assert len(indices_to_remove)!=len(soft_rationale_predictions), 'Inside get_topK_indices(), all tokens removed'
        return indices_to_remove


    test_df['to_remove_indices'] = test_df['rationales'].apply(lambda r : get_topK_indices(r[0]['soft_rationale_predictions']))

    def remove_topK_relevent_words(post_tokens, to_remove_indices):
        new_text = [v for i, v in enumerate(post_tokens) if i not in to_remove_indices]
        return " ".join(new_text)


    def get_topK_relevent_words(post_tokens, to_remove_indices):
        new_text = [post_tokens[i] for i in to_remove_indices]
        return " ".join(new_text)


    texts_after_removing = []
    for i in range(test_df.shape[0]):
        token_list = test_df.iloc[i]['commentText'].split(' ')
        to_remove_indices = test_df.iloc[i]['to_remove_indices']
        texts_after_removing.append(remove_topK_relevent_words(token_list, to_remove_indices))

    text_of_top_relevant_tokens = []
    for i in range(test_df.shape[0]):
        token_list = test_df.iloc[i]['commentText'].split(' ')
        to_remove_indices = test_df.iloc[i]['to_remove_indices']
        text_of_top_relevant_tokens.append(get_topK_relevent_words(token_list, to_remove_indices))

    test_df['text_after_removing_relevant_tokens'] = texts_after_removing
    test_df['text_of_top_relevant_tokens'] = text_of_top_relevant_tokens

    test_df.head()



    def run_model_after_removing_top_relevant_tokens(test_df):

        comprehensiveness_classification_scores  = []

        for i in tqdm(range(test_df.shape[0]), desc='getting comprehensiveness scores...'):
            text = test_df.iloc[i]['text_after_removing_relevant_tokens']

            label_proba,_, _ = model.predict(text)


            non_toxic_proba = float(label_proba[0] )
            toxic_proba = float(label_proba[1])

            label = 'non-toxic'
            if toxic_proba>non_toxic_proba:
                label = 'toxic'


            comprehensiveness_classification_scores.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})

        return comprehensiveness_classification_scores

    test_df['comprehensiveness_classification_scores'] = run_model_after_removing_top_relevant_tokens(test_df)
    test_df.head()


    #suffiency score

    def run_model_on_top_relevant_tokens(test_df):

        sufficiency_classification_scores  = []

        for i in tqdm(range(test_df.shape[0]), desc='getting sufficiency scores...'):
            
            text = test_df.iloc[i]['text_of_top_relevant_tokens']


            label_proba,_, _ = model.predict(text)
            non_toxic_proba = float(label_proba[0]) #since output_proba is a 2*1 array
            toxic_proba = float(label_proba[1])

            label = 'non-toxic'
            if toxic_proba>non_toxic_proba:
                label = 'toxic'

            if non_toxic_proba==None or toxic_proba ==None:
                print(text)
                print(i)
            sufficiency_classification_scores.append( {'non-toxic':non_toxic_proba, 'toxic': toxic_proba})

        return sufficiency_classification_scores

    sufficiency_classification_scores = run_model_on_top_relevant_tokens(test_df)
    test_df['sufficiency_classification_scores'] = sufficiency_classification_scores


    #thresholded_scores calculation
    thresholded_scores = []
    for i in range(test_df.shape[0]):
        temp_list = []
        temp_dict = {}
        temp_dict['threshold'] = 0.5
        temp_dict['comprehensiveness_classification_scores'] = test_df.iloc[i]['comprehensiveness_classification_scores']
        temp_dict['sufficiency_classification_scores'] = test_df.iloc[i]['sufficiency_classification_scores']

        temp_list.append(temp_dict)

        thresholded_scores.append(temp_list)

    test_df['thresholded_scores'] = thresholded_scores
    test_df = test_df[['annotation_id', 'rationales',  'classification', 'classification_scores', 'comprehensiveness_classification_scores', 'sufficiency_classification_scores', 'thresholded_scores']]
    print('test_df.shape: ', test_df.shape)



    # Set the path of the directory and create it if it doesn't exist
    dir_path = 'explanation_dicts'
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

    # Set the path of the output file
    output_file_path = os.path.join(dir_path, model_name + '-LIME-' + 'faithfullness_filtering-' + args.faithfullness_filtering + '.json')

    # Print the output file path to check if it's what you expect
    print("Output file path:", output_file_path)

    # Write the JSON data to the output file
    test_df.to_json(output_file_path, orient="records")

    # Print a message to confirm that the file was saved
    if os.path.isfile(output_file_path):
        print("================File saved successfully=========")
    else:
        print(" ===============File not saved=================")








# #=====================Hatexplain metrics calculation=====================




import pandas as pd
import json
from tqdm.notebook import tqdm

# !pip install more_itertools
# !pip install sentencepiece
# !pip install transformers
# !pip install ekphrasis

import more_itertools as mit
import os

from transformers import BertTokenizer
from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, AdamW, RobertaTokenizer, RobertaModel

from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

import pandas as pd
import json
from tqdm.notebook import tqdm
import more_itertools as mit
import os

from transformers import BertTokenizer
from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, AdamW, RobertaTokenizer, RobertaModel

from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.
import pandas as pd
import json
from tqdm.notebook import tqdm
import more_itertools as mit
import os

from transformers import BertTokenizer
from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, AdamW, RobertaTokenizer, RobertaModel

from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

dict_data_folder={
      '2':{'data_file':args.data_path+ 'dataset.json','class_label':'classes_two.npy'},
      '3':{'data_file': args.data_path + 'dataset.json','class_label':'classes.npy'}
}

# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). 

params = {}
params['num_classes']=2
params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']
params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']

def get_annotated_data(params):
    #temp_read = pd.read_pickle(params['data_file'])
    with open(params['data_file'], 'r') as fp:
        data = json.load(fp)
    dict_data=[]
    for key in data:
        temp={}
        temp['post_id']=key
        temp['text']=data[key]['post_tokens']
        final_label=[]
        for i in range(1,4):
            temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']
#             temp['explain'+str(i)]=data[key]['annotators'][i-1]['rationales']
            temp['target'+str(i)]=data[key]['annotators'][i-1]['target']
            temp['label'+str(i)]=data[key]['annotators'][i-1]['label']
            final_label.append(temp['label'+str(i)])

        final_label_id=max(final_label,key=final_label.count)
        temp['rationales']=data[key]['rationales']
        if data[key]['post_id']=='13851720_gab':
          print('Inside get_annotated_data()')
          print("len(temp['rationales'][0]): ", len(temp['rationales'][0]))

            
        if(params['class_names']=='classes_two.npy'):
            if(final_label.count(final_label_id)==1):
                temp['final_label']='undecided'
            else:
                if(final_label_id in ['hatespeech','offensive']):
                    final_label_id='toxic'
                else:
                    final_label_id='non-toxic'
                temp['final_label']=final_label_id

        
        else:
            if(final_label.count(final_label_id)==1):
                temp['final_label']='undecided'
            else:
                temp['final_label']=final_label_id

        
        
        
        dict_data.append(temp)    
    temp_read = pd.DataFrame(dict_data)  
    return temp_read  

def get_annotated_data(params):
    #temp_read = pd.read_pickle(params['data_file'])
    with open(params['data_file'], 'r') as fp:
        data = json.load(fp)
    dict_data=[]
    for key in data:
        temp={}
        temp['post_id']=key
        temp['text']=data[key]['post_tokens']
        final_label=[]
        for i in range(1,4):
            temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']
#             temp['explain'+str(i)]=data[key]['annotators'][i-1]['rationales']
            temp['target'+str(i)]=data[key]['annotators'][i-1]['target']
            temp['label'+str(i)]=data[key]['annotators'][i-1]['label']
            final_label.append(temp['label'+str(i)])

        final_label_id=max(final_label,key=final_label.count)
        temp['rationales']=data[key]['rationales']
            
        if(params['class_names']=='classes_two.npy'):
            if(final_label.count(final_label_id)==1):
                temp['final_label']='undecided'
            else:
                if(final_label_id in ['hatespeech','offensive']):
                    final_label_id='toxic'
                else:
                    final_label_id='non-toxic'
                temp['final_label']=final_label_id

        else:
            if(final_label.count(final_label_id)==1):
                temp['final_label']='undecided'
            else:
                temp['final_label']=final_label_id

        
        dict_data.append(temp)    
    temp_read = pd.DataFrame(dict_data)  
    return temp_read  






data_all_labelled=get_annotated_data(params)

params_data={
    'include_special':False,  #True is want to include <url> in place of urls if False will be removed
    'bert_tokens':True, #True /False
    'type_attention':'softmax', #softmax
    'set_decay':0.1,
    'majority':2,
    'max_length':128,
    'variance':10,
    'window':4,
    'alpha':0.5,
    'p_value':0.8,
    'method':'additive',
    'decay':False,
    'normalized':False,
    'not_recollect':True,
}




if(params_data['bert_tokens']):
    print('Loading BERT tokenizer...')
    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=False)
else:
    print('Loading Normal tokenizer...')
    tokenizer=None

# Load the whole dataset and get the tokenwise rationales


import re


text_processor = TextPreProcessor(
    # terms that will be normalized
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'date', 'number'],
    # terms that will be annotated
    fix_html=True,  # fix HTML tokens
    annotate={"hashtag", "allcaps", "elongated", "repeated",
        'emphasis', 'censored'},
    # corpus from which the word statistics are going to be used 
    # for word segmentation 
    segmenter="twitter", 
    
    # corpus from which the word statistics are going to be used 
    # for spell correction
    #corrector="twitter", 
    
    unpack_hashtags=True,  # perform word segmentation on hashtags
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=False,  # spell correction for elongated words
    
    # select a tokenizer. You can use SocialTokenizer, or pass your own
    # the tokenizer, should take as input a string and return a list of tokens
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    
    # list of dictionaries, for replacing tokens extracted from the text,
    # with other expressions. You can pass more than one dictionaries.
    dicts=[emoticons]
)

def custom_tokenize(sent,tokenizer,max_length=512):
    # `encode` will:
    #   (1) Tokenize the sentence.
    #   (2) Prepend the `[CLS]` token to the start.
    #   (3) Append the `[SEP]` token to the end.
    #   (4) Map tokens to their IDs.
    try:

        encoded_sent = tokenizer.encode(
                            sent,                      # Sentence to encode.
                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'
                            #max_length = max_length,
                            # This function also supports truncation and conversion
                            # to pytorch tensors, but we need to do padding, so we
                            # can't use these features :( .
                            #max_length = 128,          # Truncate all sentences.
                            #return_tensors = 'pt',     # Return pytorch tensors.
                       )

        # Add the encoded sentence to the list.

    except ValueError:
        encoded_sent = tokenizer.encode(
                            ' ',                      # Sentence to encode.
                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'
                            max_length = max_length,
                    
                       )
          ### decide what to later

    return encoded_sent

# Load the whole dataset and get the tokenwise rationales



text_processor = TextPreProcessor(
    # terms that will be normalized
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'date', 'number'],
    # terms that will be annotated
    fix_html=True,  # fix HTML tokens
    annotate={"hashtag", "allcaps", "elongated", "repeated",
        'emphasis', 'censored'},
    # corpus from which the word statistics are going to be used 
    # for word segmentation 
    segmenter="twitter", 
    
    # corpus from which the word statistics are going to be used 
    # for spell correction
    #corrector="twitter", 
    
    unpack_hashtags=True,  # perform word segmentation on hashtags
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=False,  # spell correction for elongated words
    
    # select a tokenizer. You can use SocialTokenizer, or pass your own
    # the tokenizer, should take as input a string and return a list of tokens
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    
    # list of dictionaries, for replacing tokens extracted from the text,
    # with other expressions. You can pass more than one dictionaries.
    dicts=[emoticons]
)

def custom_tokenize(sent,tokenizer,max_length=512):
    # `encode` will:
    #   (1) Tokenize the sentence.
    #   (2) Prepend the `[CLS]` token to the start.
    #   (3) Append the `[SEP]` token to the end.
    #   (4) Map tokens to their IDs.
    try:

        encoded_sent = tokenizer.encode(
                            sent,                      # Sentence to encode.
                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'
                            #max_length = max_length,
                            # This function also supports truncation and conversion
                            # to pytorch tensors, but we need to do padding, so we
                            # can't use these features :( .
                            #max_length = 128,          # Truncate all sentences.
                            #return_tensors = 'pt',     # Return pytorch tensors.
                       )

        # Add the encoded sentence to the list.

    except ValueError:
        encoded_sent = tokenizer.encode(
                            ' ',                      # Sentence to encode.
                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'
                            max_length = max_length,
                    
                       )
          ### decide what to later

    return encoded_sent


def ek_extra_preprocess(post_id, text,params,tokenizer):
    remove_words=['<allcaps>','</allcaps>','<hashtag>','</hashtag>','<elongated>','<emphasis>','<repeated>','\'','s']
    word_list=text_processor.pre_process_doc(text)

        

    if(params['include_special']):
        pass
    else:
        word_list=list(filter(lambda a: a not in remove_words, word_list)) 


    if(params['bert_tokens']):
        sent=" ".join(word_list)
        sent = re.sub(r"[<\*>]", " ",sent)
        sub_word_list = custom_tokenize(sent,tokenizer)

        return sub_word_list
    else:
        word_list=[token for token in word_list if token not in string.punctuation]
        
        return 


def returnMask(row,params,tokenizer):
    
    text_tokens=row['text']
    
    
    
    ##### a very rare corner case
    if(len(text_tokens)==0):
        text_tokens=['dummy']
        print("length of text ==0")
    #####
    
    
    mask_all= row['rationales']
    post_id = row['post_id']
    
    mask_all_temp=mask_all
    count_temp=0


    while(len(mask_all_temp)!=3):
        mask_all_temp.append([0]*len(text_tokens))
    
    word_mask_all=[]
    word_tokens_all=[]
    
    for mask in mask_all_temp:
        if(mask[0]==-1):
            mask=[0]*len(mask)
        
        
        list_pos=[]
        mask_pos=[]
        
        flag=0


        for i in range(0,len(mask)):
            if(i==0 and mask[i]==0):
                list_pos.append(0)
                mask_pos.append(0)

 
            if(flag==0 and mask[i]==1):
                mask_pos.append(1)
                list_pos.append(i)

                flag=1
                
            elif(flag==1 and mask[i]==0):
                flag=0
                mask_pos.append(0)
                list_pos.append(i)

        if(list_pos[-1]!=len(mask)):

          
          list_pos.append(len(mask))
          mask_pos.append(0)

    
        string_parts=[]
        for i in range(len(list_pos)-1):
            string_parts.append(text_tokens[list_pos[i]:list_pos[i+1]])


        if(params['bert_tokens']):
            word_tokens=[101]
            word_mask=[0]
        else:
            word_tokens=[]
            word_mask=[]

        for i in range(0,len(string_parts)):
            tokens=ek_extra_preprocess(post_id, " ".join(string_parts[i]),params,tokenizer)

            masks=[mask_pos[i]]*len(tokens)
            word_tokens+=tokens
            word_mask+=masks

        if(params['bert_tokens']):
            ### always post truncation
            word_tokens=word_tokens[0:(int(params['max_length'])-2)]
            word_mask=word_mask[0:(int(params['max_length'])-2)]
            word_tokens.append(102)
            word_mask.append(0)


        word_mask_all.append(word_mask)
        word_tokens_all.append(word_tokens)


        # if post_id=='13851720_gab':
        #   print('len(word_mask): ', len(word_mask))
        
#     for k in range(0,len(mask_all)):
#          if(mask_all[k][0]==-1):
#             word_mask_all[k] = [-1]*len(word_mask_all[k])
    if(len(mask_all)==0):
        word_mask_all=[]
    else:  
        word_mask_all=word_mask_all[0:len(mask_all)]

    return word_tokens_all[0],word_mask_all 



def get_training_data(data):
    post_ids_list=[]
    text_list=[]
    attention_list=[]
    label_list=[]
    
    final_binny_output = []
    print('total_data',len(data))
    for index,row in tqdm(data.iterrows(),total=len(data)):
        annotation=row['final_label']
        
        text=row['text']
        post_id=row['post_id']
        annotation_list=[row['label1'],row['label2'],row['label3']]
        # tokens_all = list(row['text'])
        # attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]
        
        if(annotation!= 'undecided'):
            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)
            # print(attention_masks)
            tokens_all = list(row['text'])
            attention_masks = row['rationales']
            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])

    return final_binny_output

training_data=get_training_data(data_all_labelled)

import more_itertools as mit

def find_ranges(iterable):
    """Yield range of consecutive numbers."""
    for group in mit.consecutive_groups(iterable):
        group = list(group)
        if len(group) == 1:
            yield group[0]
        else:
            yield group[0], group[-1]


            
# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py
def get_evidence(post_id, anno_text, explanations):

    output = []

    indexes = sorted([i for i, each in enumerate(explanations) if each==1])
    span_list =  find_ranges(indexes) #list(find_ranges(indexes))


    for each in span_list:
        if type(each)== int:
            start = each
            end = each+1
        elif len(each) == 2:
            start = each[0]
            end = each[1]+1
        else:
            print('error')

  
        output.append({"docid":post_id, 
              "end_sentence": -1, 
              "end_token": end, 
              "start_sentence": -1, 
              "start_token": start, 
              "text": ' '.join([str(x) for x in anno_text[start:end]])})


    return output

# To use the metrices defined in ERASER, we will have to convert the dataset
def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  
    final_output = []
    
    if save_split:
        train_fp = open(save_path+'train.jsonl', 'w')
        val_fp = open(save_path+'val.jsonl', 'w')
        test_fp = open(save_path+'test.jsonl', 'w')
            
    for tcount, eachrow in enumerate(dataset):
        
        temp = {}
        post_id = eachrow[0]
        post_class = eachrow[1]
        anno_text_list = eachrow[2]

        majority_label = eachrow[1]
        
        if majority_label=='normal':
            continue
# final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])

        all_labels = eachrow[4]
        explanations = []
        for each_explain in eachrow[3]:
            explanations.append(list(each_explain))


        # For this work, we have considered the union of explanations. Other options could be explored as well.
        if method == 'union': #if any of annotator says a token is relevant then relevant
            final_explanation = [any(each) for each in zip(*explanations)]
            final_explanation = [int(each) for each in final_explanation]
        
        if post_id=='13162665_gab':
          print(len(final_explanation))
          print(anno_text_list)   
        temp['annotation_id'] = post_id
        temp['classification'] = post_class
        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]
        temp['query'] = "What is the class?"
        temp['query_type'] = None
        final_output.append(temp)
        
        if save_split:
            if not os.path.exists(save_path+'docs'):
                os.makedirs(save_path+'docs')
            
            with open(save_path+'docs/'+post_id, 'w') as fp:
                fp.write(' '.join([str(x) for x in list(anno_text_list)]))
            
            if post_id in id_division['train']:
                train_fp.write(json.dumps(temp)+'\n')
            
            elif post_id in id_division['val']:
                val_fp.write(json.dumps(temp)+'\n')
            
            elif post_id in id_division['test']:
                test_fp.write(json.dumps(temp)+'\n')
            else:
                print(post_id)
    
    if save_split:
        train_fp.close()
        val_fp.close()
        test_fp.close()
        
    return final_output

# !rm -r Model_Eval

# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.
# with open('post_id_division_split2_seed_12345.json') as fp: 


import json

post_id_divisions_path = args.data_path+'post_id_divisions.json'
if args.split==1:
    post_id_divisions_path = args.data_path+'post_id_division_split1_seed_1234.json'

if args.split==2:
    post_id_divisions_path = args.data_path+'post_id_division_split2_seed_12345.json'




with open(post_id_divisions_path) as fp:
    id_division = json.load(fp)




import os



if not os.path.exists('Model_Eval'+rid):
    os.makedirs('Model_Eval'+rid)

method = 'union'
save_split = True
save_path = 'Model_Eval'+rid+'/'  #The dataset in Eraser Format will be stored here.
convert_to_eraser_format(training_data, method, save_split, save_path, id_division)



if not os.path.exists('explanation_result'):
    os.makedirs('explanation_result')





import json
output_file_name = output_file_path.split('.')[0]+'-metrics.json'


# Open the JSON file
with open("-".join(output_file_name.split('-')[0:-1])+'.json', 'r') as json_file:
    # Parse the JSON data to obtain a list of JSON objects
    data = json.load(json_file)

# Open a new file for writing the JSON Lines data
with open("-".join(output_file_name.split('-')[0:-1])+'.jsonl', 'w') as jsonl_file:
    # Iterate over the list of JSON objects and write each one as a string
    # separated by a newline character
    for json_obj in data:
        jsonl_file.write(json.dumps(json_obj) + '\n')


explanation_scores_file = "-".join(output_file_name.split('-')[0:-1]).split('/')[1]+'.jsonl'
model_explain_output_file = output_file_name.split('/')[1]

output_file_name = output_file_name.split('/')[1]
print('explanation_scores_file: ', explanation_scores_file)
print('model_explain_output_file: ', model_explain_output_file)


import subprocess

import os
import subprocess

env = os.environ.copy()
env['PYTHONPATH'] = './:' + env.get('PYTHONPATH', '')

subprocess.run(['python', 'metrics.py', '--split', 'test', '--strict', '--data_dir', 'Model_Eval'+rid+'/', '--results', 'explanation_dicts/'+explanation_scores_file, '--score_file', 'explanation_result/'+model_explain_output_file], env=env)


# subprocess.run(['PYTHONPATH=./:$PYTHONPATH', 'python', 'metrics.py', '--split', 'test', '--strict', '--data_dir', 'Model_Eval/', '--results', 'explanation_dicts/'+explanation_scores_file, '--score_file', 'explanation_result/'+model_explain_output_file])


import json
print('======= hatexplain metrics on: '+model_explain_output_file+'==========')
with open('explanation_result/'+model_explain_output_file) as fp:
    output_data = json.load(fp)

print('\nPlausibility')
print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])
print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])
print('AUPRC :', output_data['token_soft_metrics']['auprc'])

print('\nFaithfulness')
print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])
print('Sufficiency', output_data['classification_scores']['sufficiency'])


report_path = 'reports/'+'run_ID_'+str(rid)+'_classification_report.txt'
with open(report_path, 'a') as f:
    f.write('End '+datetime.datetime.now(IST).strftime("%c")+'\n')
    f.write('\nPlausibility\n')
    f.write('IOU F1 :'+str(output_data['iou_scores'][0]['macro']['f1'])+str('\n'))
    f.write('Token F1 :'+str(output_data['token_prf']['instance_macro']['f1'])+str('\n'))
    f.write('AUPRC :'+str(output_data['token_soft_metrics']['auprc'])+str('\n'))
    f.write('\nFaithfulness\n')
    f.write('Comprehensiveness :'+str(output_data['classification_scores']['comprehensiveness'])+str('\n'))
    f.write('Sufficiency :'+str(output_data['classification_scores']['sufficiency'])+str('\n'))
    f.write(str(output_data['iou_scores'][0]['macro']['f1'])+str('\t')+str(output_data['token_prf']['instance_macro']['f1'])+str('\t')+str(output_data['token_soft_metrics']['auprc'])+str('\t')+str(output_data['classification_scores']['comprehensiveness'])+str('\t')+str(output_data['classification_scores']['sufficiency'])+str('\n'))
    



