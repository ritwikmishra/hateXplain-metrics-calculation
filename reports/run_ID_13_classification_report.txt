=====================RUN ID:  13=======================
hatespeech-training.py --split 3 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen False --encoder_name bert-base-multilingual-cased --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 13 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: bert-base-multilingual-cased
encoder_frozen?: False
bias_in_fc?: True
cls_token?: False
Data split: 3
Thu Mar 23 05:55:54 2023

EPOCH: 1/10
Training Loss: 0.651, Training Accuracy : 0.600
Validation Loss: 0.742, Validation Accuracy : 0.596

              precision    recall  f1-score   support

           0       1.00      0.01      0.01       781
           1       0.60      1.00      0.75      1141

    accuracy                           0.60      1922
   macro avg       0.80      0.50      0.38      1922
weighted avg       0.76      0.60      0.45      1922



EPOCH: 2/10
Training Loss: 0.560, Training Accuracy : 0.706
Validation Loss: 0.711, Validation Accuracy : 0.636

              precision    recall  f1-score   support

           0       0.80      0.14      0.24       781
           1       0.62      0.98      0.76      1141

    accuracy                           0.64      1922
   macro avg       0.71      0.56      0.50      1922
weighted avg       0.69      0.64      0.55      1922



EPOCH: 4/10
Training Loss: 0.494, Training Accuracy : 0.754
Validation Loss: 0.712, Validation Accuracy : 0.652

              precision    recall  f1-score   support

           0       0.79      0.19      0.31       781
           1       0.64      0.97      0.77      1141

    accuracy                           0.65      1922
   macro avg       0.71      0.58      0.54      1922
weighted avg       0.70      0.65      0.58      1922



EPOCH: 6/10
Training Loss: 0.444, Training Accuracy : 0.792
Validation Loss: 0.720, Validation Accuracy : 0.666

              precision    recall  f1-score   support

           0       0.79      0.24      0.37       781
           1       0.65      0.96      0.77      1141

    accuracy                           0.66      1922
   macro avg       0.72      0.60      0.57      1922
weighted avg       0.70      0.66      0.61      1922



EPOCH: 7/10
Training Loss: 0.431, Training Accuracy : 0.795
Validation Loss: 0.702, Validation Accuracy : 0.674

              precision    recall  f1-score   support

           0       0.78      0.27      0.40       781
           1       0.66      0.95      0.77      1141

    accuracy                           0.67      1922
   macro avg       0.72      0.61      0.59      1922
weighted avg       0.71      0.67      0.62      1922



EPOCH: 8/10
Training Loss: 0.438, Training Accuracy : 0.791
Validation Loss: 0.685, Validation Accuracy : 0.672

              precision    recall  f1-score   support

           0       0.77      0.27      0.40       781
           1       0.65      0.94      0.77      1141

    accuracy                           0.67      1922
   macro avg       0.71      0.61      0.59      1922
weighted avg       0.70      0.67      0.62      1922



EPOCH: 9/10
Training Loss: 0.424, Training Accuracy : 0.801
Validation Loss: 0.662, Validation Accuracy : 0.681

              precision    recall  f1-score   support

           0       0.76      0.31      0.44       781
           1       0.66      0.93      0.78      1141

    accuracy                           0.68      1922
   macro avg       0.71      0.62      0.61      1922
weighted avg       0.70      0.68      0.64      1922



EPOCH: 10/10
Training Loss: 0.418, Training Accuracy : 0.805
Validation Loss: 0.649, Validation Accuracy : 0.690

              precision    recall  f1-score   support

           0       0.76      0.34      0.47       781
           1       0.67      0.93      0.78      1141

    accuracy                           0.69      1922
   macro avg       0.72      0.63      0.63      1922
weighted avg       0.71      0.69      0.65      1922


Thu Mar 23 21:25:38 2023
Testing Accuracy : 0.703
              precision    recall  f1-score   support

           0       0.80      0.36      0.50       782
           1       0.68      0.94      0.79      1142

    accuracy                           0.70      1924
   macro avg       0.74      0.65      0.64      1924
weighted avg       0.73      0.70      0.67      1924


======= hatexplain metrics on: trained_models/runID-13-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-13-checkpoint.pth --data_path data/ --encoder_name bert-base-multilingual-cased --encoder_frozen False 
Begin Sun Apr  2 13:26:02 2023
End Sun Apr  2 13:43:10 2023

Plausibility
IOU F1 :0.10007822043041285
Token F1 :0.1842159264681364
AUPRC :0.4500752194104932

Faithfulness
Comprehensiveness :0.11075336156346154
Sufficiency :0.23083720112967776
0.10007822043041285	0.1842159264681364	0.4500752194104932	0.11075336156346154	0.23083720112967776
======= hatexplain metrics on: trained_models/runID-13-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-13-checkpoint.pth --data_path data/ --encoder_name bert-base-multilingual-cased --encoder_frozen False 
Begin Sun Apr  2 13:43:15 2023
End Sun Apr  2 14:04:41 2023

Plausibility
IOU F1 :0.0991183890747014
Token F1 :0.19146421510302325
AUPRC :0.4496494917355939

Faithfulness
Comprehensiveness :0.10470192734994803
Sufficiency :0.24035443055696465
0.0991183890747014	0.19146421510302325	0.4496494917355939	0.10470192734994803	0.24035443055696465
======= hatexplain metrics on: trained_models/runID-13-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-13-checkpoint.pth --data_path data/ --encoder_name bert-base-multilingual-cased --encoder_frozen False 
Begin Sun Apr  2 14:04:46 2023
End Sun Apr  2 14:21:55 2023

Plausibility
IOU F1 :0.10457617923266967
Token F1 :0.1808138604054861
AUPRC :0.44870560558801875

Faithfulness
Comprehensiveness :0.11204924858284822
Sufficiency :0.23222915797645532
0.10457617923266967	0.1808138604054861	0.44870560558801875	0.11204924858284822	0.23222915797645532
======= hatexplain metrics on: trained_models/runID-13-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-13-checkpoint.pth --data_path data/ --encoder_name bert-base-multilingual-cased --encoder_frozen False 
Begin Sun Apr  2 14:22:00 2023
End Sun Apr  2 14:28:16 2023

Plausibility
IOU F1 :0.29504685713167245
Token F1 :0.3021200611148063
AUPRC :0.6555342599046158

Faithfulness
Comprehensiveness :0.1859199195455821
Sufficiency :-0.031221735941528068
0.29504685713167245	0.3021200611148063	0.6555342599046158	0.1859199195455821	-0.031221735941528068
Tue Apr  4 13:15:47 2023
Keep-4 Testing Accuracy : 0.712
              precision    recall  f1-score   support

           0       0.77      0.42      0.54       782
           1       0.70      0.91      0.79      1142

    accuracy                           0.71      1924
   macro avg       0.73      0.67      0.67      1924
weighted avg       0.73      0.71      0.69      1924


