=====================RUN ID:  29=======================
hatespeech-training.py --split 2 --max_len 300 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen False --encoder_name bert-base-cased --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 29 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=230400, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: bert-base-cased
encoder_frozen?: False
bias_in_fc?: True
cls_token?: False
Data split: 2
Sat Apr 22 06:01:41 2023

EPOCH: 1/10
Training Loss: 0.638, Training Accuracy : 0.617
Validation Loss: 0.645, Validation Accuracy : 0.642

              precision    recall  f1-score   support

           0       0.72      0.11      0.18       739
           1       0.64      0.97      0.77      1185

    accuracy                           0.64      1924
   macro avg       0.68      0.54      0.48      1924
weighted avg       0.67      0.64      0.54      1924



EPOCH: 2/10
Training Loss: 0.540, Training Accuracy : 0.725
Validation Loss: 0.616, Validation Accuracy : 0.677

              precision    recall  f1-score   support

           0       0.76      0.23      0.36       739
           1       0.67      0.95      0.78      1185

    accuracy                           0.68      1924
   macro avg       0.71      0.59      0.57      1924
weighted avg       0.70      0.68      0.62      1924



EPOCH: 3/10
Training Loss: 0.488, Training Accuracy : 0.761
Validation Loss: 0.599, Validation Accuracy : 0.699

              precision    recall  f1-score   support

           0       0.77      0.31      0.44       739
           1       0.69      0.94      0.79      1185

    accuracy                           0.70      1924
   macro avg       0.73      0.62      0.62      1924
weighted avg       0.72      0.70      0.66      1924



EPOCH: 4/10
Training Loss: 0.462, Training Accuracy : 0.777
Validation Loss: 0.592, Validation Accuracy : 0.706

              precision    recall  f1-score   support

           0       0.77      0.34      0.47       739
           1       0.69      0.94      0.80      1185

    accuracy                           0.71      1924
   macro avg       0.73      0.64      0.63      1924
weighted avg       0.72      0.71      0.67      1924



EPOCH: 5/10
Training Loss: 0.440, Training Accuracy : 0.793
Validation Loss: 0.591, Validation Accuracy : 0.714

              precision    recall  f1-score   support

           0       0.78      0.35      0.49       739
           1       0.70      0.94      0.80      1185

    accuracy                           0.71      1924
   macro avg       0.74      0.65      0.64      1924
weighted avg       0.73      0.71      0.68      1924



EPOCH: 6/10
Training Loss: 0.432, Training Accuracy : 0.797
Validation Loss: 0.576, Validation Accuracy : 0.719

              precision    recall  f1-score   support

           0       0.77      0.38      0.51       739
           1       0.71      0.93      0.80      1185

    accuracy                           0.72      1924
   macro avg       0.74      0.66      0.66      1924
weighted avg       0.73      0.72      0.69      1924



EPOCH: 7/10
Training Loss: 0.423, Training Accuracy : 0.801
Validation Loss: 0.564, Validation Accuracy : 0.726

              precision    recall  f1-score   support

           0       0.76      0.42      0.54       739
           1       0.72      0.92      0.80      1185

    accuracy                           0.73      1924
   macro avg       0.74      0.67      0.67      1924
weighted avg       0.73      0.73      0.70      1924



EPOCH: 8/10
Training Loss: 0.410, Training Accuracy : 0.811
Validation Loss: 0.562, Validation Accuracy : 0.736

              precision    recall  f1-score   support

           0       0.76      0.45      0.57       739
           1       0.73      0.91      0.81      1185

    accuracy                           0.74      1924
   macro avg       0.74      0.68      0.69      1924
weighted avg       0.74      0.74      0.72      1924



EPOCH: 9/10
Training Loss: 0.408, Training Accuracy : 0.813
Validation Loss: 0.554, Validation Accuracy : 0.742

              precision    recall  f1-score   support

           0       0.76      0.48      0.59       739
           1       0.74      0.90      0.81      1185

    accuracy                           0.74      1924
   macro avg       0.75      0.69      0.70      1924
weighted avg       0.74      0.74      0.73      1924



EPOCH: 10/10
Training Loss: 0.424, Training Accuracy : 0.800
Validation Loss: 0.508, Validation Accuracy : 0.749

              precision    recall  f1-score   support

           0       0.71      0.58      0.64       739
           1       0.77      0.85      0.81      1185

    accuracy                           0.75      1924
   macro avg       0.74      0.72      0.72      1924
weighted avg       0.75      0.75      0.74      1924


Sat Apr 22 07:25:22 2023
Testing Accuracy : 0.736
              precision    recall  f1-score   support

           0       0.70      0.57      0.63       762
           1       0.75      0.84      0.79      1160

    accuracy                           0.74      1922
   macro avg       0.73      0.71      0.71      1922
weighted avg       0.73      0.74      0.73      1922


======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --max_len 300 --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Sat Apr 22 14:01:25 2023
End Sat Apr 22 14:47:02 2023

Plausibility
IOU F1 :0.2680273875704239
Token F1 :0.2933587187044738
AUPRC :0.6534879671905951

Faithfulness
Comprehensiveness :-0.027624861381113423
Sufficiency :-0.11324669173059314
0.2680273875704239	0.2933587187044738	0.6534879671905951	-0.027624861381113423	-0.11324669173059314
======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --max_len 300 --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Sun Apr 23 07:39:35 2023
End Sun Apr 23 08:48:49 2023

Plausibility
IOU F1 :0.10700089570928605
Token F1 :0.18472808298276192
AUPRC :0.47081477337531696

Faithfulness
Comprehensiveness :0.11959598401409989
Sufficiency :0.21776853792101977
0.10700089570928605	0.18472808298276192	0.47081477337531696	0.11959598401409989	0.21776853792101977
======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --max_len 300 --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Sun Apr 23 08:49:01 2023
======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --max_len 300 --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Sun Apr 23 09:22:24 2023
End Sun Apr 23 09:53:22 2023

Plausibility
IOU F1 :0.10869852631067801
Token F1 :0.1785701834440803
AUPRC :0.46980805373668366

Faithfulness
Comprehensiveness :0.12056879208397503
Sufficiency :0.22513073701508843
0.10869852631067801	0.1785701834440803	0.46980805373668366	0.12056879208397503	0.22513073701508843
End Sun Apr 23 10:35:39 2023

Plausibility
IOU F1 :0.10883921100495891
Token F1 :0.17890656251738443
AUPRC :0.46762703777728953

Faithfulness
Comprehensiveness :0.11822816940775235
Sufficiency :0.2257968586609261
0.10883921100495891	0.17890656251738443	0.46762703777728953	0.11822816940775235	0.2257968586609261
Mon Apr 24 18:03:57 2023
Keep-k = 1
Testing Accuracy : 0.725
              precision    recall  f1-score   support

           0       0.68      0.58      0.62       762
           1       0.75      0.82      0.78      1160

    accuracy                           0.72      1922
   macro avg       0.71      0.70      0.70      1922
weighted avg       0.72      0.72      0.72      1922


Mon Apr 24 18:05:45 2023
Keep-k = 2
Testing Accuracy : 0.728
              precision    recall  f1-score   support

           0       0.69      0.57      0.63       762
           1       0.75      0.83      0.79      1160

    accuracy                           0.73      1922
   macro avg       0.72      0.70      0.71      1922
weighted avg       0.72      0.73      0.72      1922


======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics2.py --method shap --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --drop_out 0.4 --encoder_frozen False --add_cls_sep_tokens False --max_len 300 
Begin Sat Apr 29 23:59:42 2023
End Sun Apr 30 04:02:20 2023

Plausibility
IOU F1 :0.25788034053689374
Token F1 :0.31605651625850006
AUPRC :0.6488072502131953

Faithfulness
Comprehensiveness :0.28184308749885534
Sufficiency :0.11567547098288242
0.25788034053689374	0.31605651625850006	0.6488072502131953	0.28184308749885534	0.11567547098288242
======= hatexplain metrics on: trained_models/runID-29-checkpoint.pth==========
ajeet-calculate-metrics2.py --method lime2 --faithfullness_filtering top-k --split 2 --model_path trained_models/runID-29-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --drop_out 0.4 --encoder_frozen False --add_cls_sep_tokens False --max_len 300 
Begin Sun Apr 30 12:27:40 2023
