=====================RUN ID:  32=======================
hatespeech-training.py --split 3 --max_len 300 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen True --encoder_name bert-base-cased --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 32 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=230400, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: bert-base-cased
encoder_frozen?: True
bias_in_fc?: True
cls_token?: False
Data split: 3
Sat Apr 22 09:40:43 2023

EPOCH: 1/10
Training Loss: 0.650, Training Accuracy : 0.610
Validation Loss: 0.699, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 2/10
Training Loss: 0.619, Training Accuracy : 0.657
Validation Loss: 0.682, Validation Accuracy : 0.608

              precision    recall  f1-score   support

           0       0.91      0.04      0.07       781
           1       0.60      1.00      0.75      1141

    accuracy                           0.61      1922
   macro avg       0.75      0.52      0.41      1922
weighted avg       0.73      0.61      0.47      1922



EPOCH: 3/10
Training Loss: 0.602, Training Accuracy : 0.673
Validation Loss: 0.672, Validation Accuracy : 0.612

              precision    recall  f1-score   support

           0       0.79      0.06      0.11       781
           1       0.61      0.99      0.75      1141

    accuracy                           0.61      1922
   macro avg       0.70      0.52      0.43      1922
weighted avg       0.68      0.61      0.49      1922



EPOCH: 4/10
Training Loss: 0.589, Training Accuracy : 0.685
Validation Loss: 0.667, Validation Accuracy : 0.621

              precision    recall  f1-score   support

           0       0.82      0.08      0.15       781
           1       0.61      0.99      0.76      1141

    accuracy                           0.62      1922
   macro avg       0.72      0.54      0.45      1922
weighted avg       0.70      0.62      0.51      1922



EPOCH: 5/10
Training Loss: 0.581, Training Accuracy : 0.693
Validation Loss: 0.664, Validation Accuracy : 0.626

              precision    recall  f1-score   support

           0       0.80      0.10      0.18       781
           1       0.62      0.98      0.76      1141

    accuracy                           0.62      1922
   macro avg       0.71      0.54      0.47      1922
weighted avg       0.69      0.62      0.52      1922



EPOCH: 6/10
Training Loss: 0.576, Training Accuracy : 0.696
Validation Loss: 0.656, Validation Accuracy : 0.635

              precision    recall  f1-score   support

           0       0.80      0.13      0.23       781
           1       0.62      0.98      0.76      1141

    accuracy                           0.63      1922
   macro avg       0.71      0.55      0.49      1922
weighted avg       0.70      0.63      0.54      1922



EPOCH: 7/10
Training Loss: 0.570, Training Accuracy : 0.700
Validation Loss: 0.651, Validation Accuracy : 0.644

              precision    recall  f1-score   support

           0       0.81      0.16      0.27       781
           1       0.63      0.97      0.76      1141

    accuracy                           0.64      1922
   macro avg       0.72      0.57      0.51      1922
weighted avg       0.70      0.64      0.56      1922



EPOCH: 8/10
Training Loss: 0.567, Training Accuracy : 0.705
Validation Loss: 0.648, Validation Accuracy : 0.646

              precision    recall  f1-score   support

           0       0.79      0.17      0.28       781
           1       0.63      0.97      0.76      1141

    accuracy                           0.65      1922
   macro avg       0.71      0.57      0.52      1922
weighted avg       0.70      0.65      0.57      1922



EPOCH: 9/10
Training Loss: 0.570, Training Accuracy : 0.703
Validation Loss: 0.640, Validation Accuracy : 0.651

              precision    recall  f1-score   support

           0       0.76      0.20      0.32       781
           1       0.64      0.96      0.76      1141

    accuracy                           0.65      1922
   macro avg       0.70      0.58      0.54      1922
weighted avg       0.69      0.65      0.58      1922



EPOCH: 10/10
Training Loss: 0.586, Training Accuracy : 0.685
Validation Loss: 0.602, Validation Accuracy : 0.662

              precision    recall  f1-score   support

           0       0.64      0.38      0.47       781
           1       0.67      0.86      0.75      1141

    accuracy                           0.66      1922
   macro avg       0.65      0.62      0.61      1922
weighted avg       0.66      0.66      0.64      1922


Sat Apr 22 10:14:17 2023
Testing Accuracy : 0.672
              precision    recall  f1-score   support

           0       0.66      0.40      0.50       782
           1       0.68      0.86      0.76      1142

    accuracy                           0.67      1924
   macro avg       0.67      0.63      0.63      1924
weighted avg       0.67      0.67      0.65      1924


======= hatexplain metrics on: trained_models/runID-32-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --max_len 300 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-32-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Sun Apr 23 05:46:07 2023
End Sun Apr 23 07:19:54 2023

Plausibility
IOU F1 :0.14106503292131897
Token F1 :0.23207909416297404
AUPRC :0.4881704843141979

Faithfulness
Comprehensiveness :0.08431071117536383
Sufficiency :0.09469065638549895
0.14106503292131897	0.23207909416297404	0.4881704843141979	0.08431071117536383	0.09469065638549895
======= hatexplain metrics on: trained_models/runID-32-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --max_len 300 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-32-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Sun Apr 23 07:20:07 2023
End Sun Apr 23 08:23:54 2023

Plausibility
IOU F1 :0.19438506401625424
Token F1 :0.2457414460893676
AUPRC :0.5728961872637186

Faithfulness
Comprehensiveness :0.032270350142047814
Sufficiency :-0.0892094639206341
0.19438506401625424	0.2457414460893676	0.5728961872637186	0.032270350142047814	-0.0892094639206341
======= hatexplain metrics on: trained_models/runID-32-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --max_len 300 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-32-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Sun Apr 23 22:12:26 2023
End Sun Apr 23 23:01:50 2023

Plausibility
IOU F1 :0.1380716021913255
Token F1 :0.23419125162277296
AUPRC :0.490698667844242

Faithfulness
Comprehensiveness :0.07830708508030144
Sufficiency :0.09603951072068607
0.1380716021913255	0.23419125162277296	0.490698667844242	0.07830708508030144	0.09603951072068607
======= hatexplain metrics on: trained_models/runID-32-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --max_len 300 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-32-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Sun Apr 23 23:01:57 2023
End Sun Apr 23 23:48:12 2023

Plausibility
IOU F1 :0.1390774505843491
Token F1 :0.23206960017780204
AUPRC :0.49136662538816744

Faithfulness
Comprehensiveness :0.08172286523393972
Sufficiency :0.09337419926704782
0.1390774505843491	0.23206960017780204	0.49136662538816744	0.08172286523393972	0.09337419926704782
Mon Apr 24 18:08:56 2023
Keep-k = 1
Testing Accuracy : 0.596
              precision    recall  f1-score   support

           0       0.52      0.09      0.15       782
           1       0.60      0.94      0.73      1142

    accuracy                           0.60      1924
   macro avg       0.56      0.52      0.44      1924
weighted avg       0.57      0.60      0.50      1924


Mon Apr 24 18:10:39 2023
Keep-k = 2
Testing Accuracy : 0.615
              precision    recall  f1-score   support

           0       0.73      0.08      0.15       782
           1       0.61      0.98      0.75      1142

    accuracy                           0.61      1924
   macro avg       0.67      0.53      0.45      1924
weighted avg       0.66      0.61      0.50      1924


======= hatexplain metrics on: trained_models/runID-32-checkpoint.pth==========
ajeet-calculate-metrics2.py --method shap --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-32-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --drop_out 0.4 --encoder_frozen True --add_cls_sep_tokens False --max_len 300 
Begin Sun Apr 30 02:45:24 2023
End Sun Apr 30 05:30:12 2023

Plausibility
IOU F1 :0.17449533158090091
Token F1 :0.26172046055587683
AUPRC :0.5613513739774927

Faithfulness
Comprehensiveness :0.10308352785176714
Sufficiency :0.08531991815192308
0.17449533158090091	0.26172046055587683	0.5613513739774927	0.10308352785176714	0.08531991815192308
