=====================RUN ID:  2=======================
hatespeech-training.py --split 1 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen False --encoder_name bert-base-cased --data_path data/ --checkpoint_path . --message changed LR for ft+bert and changed dropout --dummy False --run_ID 2 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : changed LR for ft+bert and changed dropout
FINE TUNING LAYERS: 

flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.4, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)
Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: bert-base-cased
encoder_frozen?: False
bias_in_fc?: True
cls_token?: False
Data split: 1

EPOCH: 1/10
Training Loss: 0.644, Training Accuracy : 0.598
Validation Loss: 0.705, Validation Accuracy : 0.591
              precision    recall  f1-score   support

           0       0.82      0.01      0.02       799
           1       0.59      1.00      0.74      1125

    accuracy                           0.59      1924
   macro avg       0.70      0.50      0.38      1924
weighted avg       0.68      0.59      0.44      1924



EPOCH: 2/10
Training Loss: 0.557, Training Accuracy : 0.708
Validation Loss: 0.663, Validation Accuracy : 0.657
              precision    recall  f1-score   support

           0       0.81      0.22      0.35       799
           1       0.63      0.96      0.77      1125

    accuracy                           0.65      1924
   macro avg       0.72      0.59      0.56      1924
weighted avg       0.71      0.65      0.59      1924



EPOCH: 3/10
Training Loss: 0.499, Training Accuracy : 0.754
Validation Loss: 0.630, Validation Accuracy : 0.680
              precision    recall  f1-score   support

           0       0.78      0.31      0.44       799
           1       0.66      0.94      0.77      1125

    accuracy                           0.68      1924
   macro avg       0.72      0.62      0.61      1924
weighted avg       0.71      0.68      0.64      1924



EPOCH: 4/10
Training Loss: 0.468, Training Accuracy : 0.774
Validation Loss: 0.629, Validation Accuracy : 0.684
              precision    recall  f1-score   support

           0       0.79      0.32      0.46       799
           1       0.66      0.94      0.77      1125

    accuracy                           0.68      1924
   macro avg       0.72      0.63      0.61      1924
weighted avg       0.71      0.68      0.64      1924



EPOCH: 5/10
Training Loss: 0.454, Training Accuracy : 0.786
Validation Loss: 0.611, Validation Accuracy : 0.693
              precision    recall  f1-score   support

           0       0.77      0.37      0.50       799
           1       0.67      0.92      0.78      1125

    accuracy                           0.69      1924
   macro avg       0.72      0.64      0.64      1924
weighted avg       0.71      0.69      0.66      1924



EPOCH: 6/10
Training Loss: 0.439, Training Accuracy : 0.790
Validation Loss: 0.615, Validation Accuracy : 0.695
              precision    recall  f1-score   support

           0       0.76      0.38      0.51       799
           1       0.67      0.91      0.78      1125

    accuracy                           0.69      1924
   macro avg       0.72      0.65      0.64      1924
weighted avg       0.71      0.69      0.66      1924



EPOCH: 7/10
Training Loss: 0.427, Training Accuracy : 0.800
Validation Loss: 0.605, Validation Accuracy : 0.703
              precision    recall  f1-score   support

           0       0.76      0.40      0.53       799
           1       0.68      0.91      0.78      1125

    accuracy                           0.70      1924
   macro avg       0.72      0.66      0.65      1924
weighted avg       0.72      0.70      0.68      1924



EPOCH: 8/10
Training Loss: 0.422, Training Accuracy : 0.800
Validation Loss: 0.600, Validation Accuracy : 0.705
              precision    recall  f1-score   support

           0       0.75      0.42      0.54       799
           1       0.69      0.90      0.78      1125

    accuracy                           0.70      1924
   macro avg       0.72      0.66      0.66      1924
weighted avg       0.71      0.70      0.68      1924



EPOCH: 9/10
Training Loss: 0.420, Training Accuracy : 0.802
Validation Loss: 0.599, Validation Accuracy : 0.707
              precision    recall  f1-score   support

           0       0.75      0.43      0.55       799
           1       0.69      0.90      0.78      1125

    accuracy                           0.70      1924
   macro avg       0.72      0.67      0.66      1924
weighted avg       0.72      0.70      0.68      1924



EPOCH: 10/10
Training Loss: 0.440, Training Accuracy : 0.790
Validation Loss: 0.536, Validation Accuracy : 0.742
              precision    recall  f1-score   support

           0       0.72      0.60      0.66       799
           1       0.75      0.84      0.79      1125

    accuracy                           0.74      1924
   macro avg       0.74      0.72      0.72      1924
weighted avg       0.74      0.74      0.73      1924


Testing Accuracy : 0.742
              precision    recall  f1-score   support

           0       0.73      0.58      0.65       787
           1       0.75      0.85      0.80      1132

    accuracy                           0.74      1919
   macro avg       0.74      0.72      0.72      1919
weighted avg       0.74      0.74      0.74      1919




======= hatexplain metrics on: trained_models/runID-2-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-2-checkpoint.pth --data_path data/ --encoder_name bert-base-cased 
Begin Fri Mar 24 13:49:57 2023
End Fri Mar 24 14:06:22 2023

Plausibility
IOU F1 :0.13666652550029773
Token F1 :0.22846141153422617
AUPRC :0.5004654566736996

Faithfulness
Comprehensiveness :-0.004191705559937565
Sufficiency :-0.0035495499426638918
0.13666652550029773	0.22846141153422617	0.5004654566736996	-0.004191705559937565	-0.0035495499426638918

======= hatexplain metrics on: trained_models/runID-2-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-2-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Fri Mar 24 15:16:03 2023
End Fri Mar 24 15:42:27 2023

Plausibility
IOU F1 :0.13187793694775107
Token F1 :0.22624467855060867
AUPRC :0.4989661330629073

Faithfulness
Comprehensiveness :-0.00602163690712799
Sufficiency :-7.147447362122725e-05
0.13187793694775107	0.22624467855060867	0.4989661330629073	-0.00602163690712799	-7.147447362122725e-05
======= hatexplain metrics on: trained_models/runID-2-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-2-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Fri Mar 24 15:42:31 2023
End Fri Mar 24 15:58:46 2023

Plausibility
IOU F1 :0.13933470411656165
Token F1 :0.2235828920604309
AUPRC :0.5029663445422343

Faithfulness
Comprehensiveness :-0.0023683824354838695
Sufficiency :-0.005759245198178981
0.13933470411656165	0.2235828920604309	0.5029663445422343	-0.0023683824354838695	-0.005759245198178981
======= hatexplain metrics on: trained_models/runID-2-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-2-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Fri Mar 24 15:58:51 2023
End Fri Mar 24 16:04:41 2023

Plausibility
IOU F1 :0.24604646029916585
Token F1 :0.28487463375085176
AUPRC :0.6363544323116933

Faithfulness
Comprehensiveness :-0.002241310387200834
Sufficiency :-0.017088447274141518
0.24604646029916585	0.28487463375085176	0.6363544323116933	-0.002241310387200834	-0.017088447274141518
Tue Apr  4 12:04:43 2023
Keep-4 Testing Accuracy : 0.740
              precision    recall  f1-score   support

           0       0.76      0.54      0.63       787
           1       0.73      0.88      0.80      1132

    accuracy                           0.74      1919
   macro avg       0.74      0.71      0.71      1919
weighted avg       0.74      0.74      0.73      1919


======= hatexplain metrics on: trained_models/runID-2-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_features 10 --lime_num_samples 1500 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-2-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen False 
Begin Fri Apr  7 23:16:26 2023
End Sat Apr  8 00:09:32 2023

Plausibility
IOU F1 :0.2622959150483544
Token F1 :0.2833993276429537
AUPRC :0.6506916808039052

Faithfulness
Comprehensiveness :-0.05737498172237253
Sufficiency :-0.05440106548100936
0.2622959150483544	0.2833993276429537	0.6506916808039052	-0.05737498172237253	-0.05440106548100936
Tue Apr 11 14:13:49 2023

keep 3
Testing Accuracy : 0.740
              precision    recall  f1-score   support

           0       0.76      0.53      0.63       789
           1       0.73      0.88      0.80      1133

    accuracy                           0.74      1922
   macro avg       0.75      0.71      0.71      1922
weighted avg       0.74      0.74      0.73      1922


Tue Apr 11 14:58:38 2023
Testing Accuracy : 0.743
              precision    recall  f1-score   support

           0       0.74      0.58      0.65       789
           1       0.75      0.85      0.80      1133

    accuracy                           0.74      1922
   macro avg       0.74      0.72      0.72      1922
weighted avg       0.74      0.74      0.74      1922


Tue Apr 11 15:41:21 2023
Testing Accuracy : 0.743
              precision    recall  f1-score   support

           0       0.73      0.59      0.65       789
           1       0.75      0.85      0.80      1133

    accuracy                           0.74      1922
   macro avg       0.74      0.72      0.72      1922
weighted avg       0.74      0.74      0.74      1922


Fri Apr 14 12:05:53 2023
Keep-k = 1
Testing Accuracy : 0.740
              precision    recall  f1-score   support

           0       0.67      0.70      0.69       789
           1       0.79      0.76      0.78      1133

    accuracy                           0.74      1922
   macro avg       0.73      0.73      0.73      1922
weighted avg       0.74      0.74      0.74      1922


Tue Apr 18 15:09:11 2023
Keep-k = 2
Testing Accuracy : 0.735
              precision    recall  f1-score   support

           0       0.75      0.53      0.62       789
           1       0.73      0.88      0.80      1133

    accuracy                           0.73      1922
   macro avg       0.74      0.70      0.71      1922
weighted avg       0.74      0.73      0.72      1922


Tue Apr 18 16:51:09 2023
Keep-k = 3
Testing Accuracy : 0.724
              precision    recall  f1-score   support

           0       0.76      0.48      0.59       789
           1       0.71      0.90      0.79      1133

    accuracy                           0.72      1922
   macro avg       0.74      0.69      0.69      1922
weighted avg       0.73      0.72      0.71      1922


Thu Apr 20 20:58:56 2023
Keep-k = 4
Testing Accuracy : 0.739
              precision    recall  f1-score   support

           0       0.67      0.70      0.69       789
           1       0.78      0.77      0.77      1133

    accuracy                           0.74      1922
   macro avg       0.73      0.73      0.73      1922
weighted avg       0.74      0.74      0.74      1922


