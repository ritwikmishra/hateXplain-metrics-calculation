=====================RUN ID:  4=======================
hatespeech-training.py --split 1 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen True --encoder_name bert-base-cased --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 4 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 

flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.4, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)
Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: bert-base-cased
encoder_frozen?: True
bias_in_fc?: True
cls_token?: False
Data split: 1
Wed Mar 22 19:52:20 2023

EPOCH: 1/10
Training Loss: 0.652, Training Accuracy : 0.598
Validation Loss: 0.695, Validation Accuracy : 0.588
              precision    recall  f1-score   support

           0       1.00      0.00      0.00       799
           1       0.59      1.00      0.74      1125

    accuracy                           0.59      1924
   macro avg       0.79      0.50      0.37      1924
weighted avg       0.76      0.59      0.43      1924



EPOCH: 2/10
Training Loss: 0.625, Training Accuracy : 0.648
Validation Loss: 0.679, Validation Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.85      0.02      0.04       799
           1       0.59      1.00      0.74      1125

    accuracy                           0.59      1924
   macro avg       0.72      0.51      0.39      1924
weighted avg       0.70      0.59      0.45      1924



EPOCH: 3/10
Training Loss: 0.607, Training Accuracy : 0.671
Validation Loss: 0.674, Validation Accuracy : 0.602
              precision    recall  f1-score   support

           0       0.77      0.05      0.09       799
           1       0.59      0.99      0.74      1125

    accuracy                           0.60      1924
   macro avg       0.68      0.52      0.42      1924
weighted avg       0.67      0.60      0.47      1924



EPOCH: 4/10
Training Loss: 0.596, Training Accuracy : 0.677
Validation Loss: 0.668, Validation Accuracy : 0.607
              precision    recall  f1-score   support

           0       0.76      0.07      0.13       799
           1       0.60      0.98      0.74      1125

    accuracy                           0.60      1924
   macro avg       0.68      0.53      0.44      1924
weighted avg       0.67      0.60      0.49      1924



EPOCH: 5/10
Training Loss: 0.589, Training Accuracy : 0.682
Validation Loss: 0.663, Validation Accuracy : 0.608
              precision    recall  f1-score   support

           0       0.73      0.08      0.14       799
           1       0.60      0.98      0.74      1125

    accuracy                           0.61      1924
   macro avg       0.67      0.53      0.44      1924
weighted avg       0.65      0.61      0.49      1924



EPOCH: 6/10
Training Loss: 0.580, Training Accuracy : 0.696
Validation Loss: 0.659, Validation Accuracy : 0.611
              precision    recall  f1-score   support

           0       0.72      0.10      0.17       799
           1       0.60      0.97      0.74      1125

    accuracy                           0.61      1924
   macro avg       0.66      0.53      0.46      1924
weighted avg       0.65      0.61      0.51      1924



EPOCH: 7/10
Training Loss: 0.577, Training Accuracy : 0.697
Validation Loss: 0.653, Validation Accuracy : 0.621
              precision    recall  f1-score   support

           0       0.75      0.13      0.21       799
           1       0.61      0.97      0.75      1125

    accuracy                           0.62      1924
   macro avg       0.68      0.55      0.48      1924
weighted avg       0.67      0.62      0.53      1924



EPOCH: 8/10
Training Loss: 0.579, Training Accuracy : 0.695
Validation Loss: 0.648, Validation Accuracy : 0.625
              precision    recall  f1-score   support

           0       0.74      0.14      0.24       799
           1       0.61      0.97      0.75      1125

    accuracy                           0.62      1924
   macro avg       0.68      0.55      0.49      1924
weighted avg       0.67      0.62      0.54      1924



EPOCH: 9/10
Training Loss: 0.578, Training Accuracy : 0.698
Validation Loss: 0.639, Validation Accuracy : 0.628
              precision    recall  f1-score   support

           0       0.71      0.17      0.27       799
           1       0.62      0.95      0.75      1125

    accuracy                           0.63      1924
   macro avg       0.66      0.56      0.51      1924
weighted avg       0.65      0.63      0.55      1924



EPOCH: 10/10
Training Loss: 0.592, Training Accuracy : 0.684
Validation Loss: 0.610, Validation Accuracy : 0.665
              precision    recall  f1-score   support

           0       0.65      0.40      0.50       799
           1       0.67      0.85      0.75      1125

    accuracy                           0.66      1924
   macro avg       0.66      0.62      0.62      1924
weighted avg       0.66      0.66      0.64      1924


Wed Mar 22 20:55:31 2023
Testing Accuracy : 0.672
              precision    recall  f1-score   support

           0       0.67      0.39      0.50       787
           1       0.67      0.87      0.76      1132

    accuracy                           0.67      1919
   macro avg       0.67      0.63      0.63      1919
weighted avg       0.67      0.67      0.65      1919


======= hatexplain metrics on: trained_models/runID-4-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-4-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Fri Mar 24 16:04:46 2023
End Fri Mar 24 16:20:54 2023

Plausibility
IOU F1 :0.1124258814665301
Token F1 :0.23579103408535787
AUPRC :0.49133094127934845

Faithfulness
Comprehensiveness :-0.0009072436005723181
Sufficiency :0.025615184630905306
0.1124258814665301	0.23579103408535787	0.49133094127934845	-0.0009072436005723181	0.025615184630905306
======= hatexplain metrics on: trained_models/runID-4-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-4-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Fri Mar 24 16:20:58 2023
End Fri Mar 24 16:41:26 2023

Plausibility
IOU F1 :0.1243260248116542
Token F1 :0.23414598283217372
AUPRC :0.4913924248406473

Faithfulness
Comprehensiveness :-0.0007297574368366255
Sufficiency :0.024704849575078045
0.1243260248116542	0.23414598283217372	0.4913924248406473	-0.0007297574368366255	0.024704849575078045
======= hatexplain metrics on: trained_models/runID-4-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-4-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Fri Mar 24 16:41:30 2023
End Fri Mar 24 16:58:02 2023

Plausibility
IOU F1 :0.12367445438708531
Token F1 :0.23634517476640723
AUPRC :0.4925514039275874

Faithfulness
Comprehensiveness :0.0010311515149843923
Sufficiency :0.024054408522944846
0.12367445438708531	0.23634517476640723	0.4925514039275874	0.0010311515149843923	0.024054408522944846
======= hatexplain metrics on: trained_models/runID-4-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-4-checkpoint.pth --data_path data/ --encoder_name bert-base-cased --encoder_frozen True 
Begin Fri Mar 24 16:58:07 2023
End Fri Mar 24 17:04:19 2023

Plausibility
IOU F1 :0.15812723596304049
Token F1 :0.25078665643965403
AUPRC :0.548857778336522

Faithfulness
Comprehensiveness :0.02470333838439126
Sufficiency :-0.018534175005098855
0.15812723596304049	0.25078665643965403	0.548857778336522	0.02470333838439126	-0.018534175005098855
Tue Apr  4 12:07:17 2023
Keep-4 Testing Accuracy : 0.606
              precision    recall  f1-score   support

           0       0.86      0.05      0.09       787
           1       0.60      0.99      0.75      1132

    accuracy                           0.61      1919
   macro avg       0.73      0.52      0.42      1919
weighted avg       0.71      0.61      0.48      1919


