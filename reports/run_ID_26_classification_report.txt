=====================RUN ID:  26=======================
hatespeech-training.py --split 3 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen True --encoder_name xlm-roberta-base --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 26 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: xlm-roberta-base
encoder_frozen?: True
bias_in_fc?: True
cls_token?: False
Data split: 3
Thu Mar 23 22:31:37 2023

EPOCH: 1/10
Training Loss: 0.666, Training Accuracy : 0.561
Validation Loss: 0.692, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.74      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 2/10
Training Loss: 0.659, Training Accuracy : 0.602
Validation Loss: 0.689, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 3/10
Training Loss: 0.652, Training Accuracy : 0.619
Validation Loss: 0.683, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 4/10
Training Loss: 0.644, Training Accuracy : 0.629
Validation Loss: 0.680, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 5/10
Training Loss: 0.641, Training Accuracy : 0.636
Validation Loss: 0.675, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 6/10
Training Loss: 0.637, Training Accuracy : 0.640
Validation Loss: 0.671, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 7/10
Training Loss: 0.635, Training Accuracy : 0.640
Validation Loss: 0.668, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 8/10
Training Loss: 0.635, Training Accuracy : 0.636
Validation Loss: 0.662, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 9/10
Training Loss: 0.638, Training Accuracy : 0.633
Validation Loss: 0.652, Validation Accuracy : 0.595

              precision    recall  f1-score   support

           0       1.00      0.01      0.01       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.60      1922
   macro avg       0.80      0.50      0.38      1922
weighted avg       0.76      0.60      0.45      1922



EPOCH: 10/10
Training Loss: 0.643, Training Accuracy : 0.623
Validation Loss: 0.646, Validation Accuracy : 0.622

              precision    recall  f1-score   support

           0       0.78      0.09      0.17       781
           1       0.61      0.98      0.75      1141

    accuracy                           0.62      1922
   macro avg       0.69      0.54      0.46      1922
weighted avg       0.68      0.62      0.52      1922


Thu Mar 23 22:47:08 2023
Testing Accuracy : 0.620
              precision    recall  f1-score   support

           0       0.75      0.10      0.17       782
           1       0.61      0.98      0.75      1142

    accuracy                           0.62      1924
   macro avg       0.68      0.54      0.46      1924
weighted avg       0.67      0.62      0.52      1924


======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Sun Apr  2 18:37:54 2023
End Sun Apr  2 18:56:51 2023

Plausibility
IOU F1 :0.09805980804163068
Token F1 :0.216674896559376
AUPRC :0.4636383853633494

Faithfulness
Comprehensiveness :0.018127776930197504
Sufficiency :0.020835826294594597
0.09805980804163068	0.216674896559376	0.4636383853633494	0.018127776930197504	0.020835826294594597
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Sun Apr  2 18:56:56 2023
End Sun Apr  2 19:18:56 2023

Plausibility
IOU F1 :0.10379374161152372
Token F1 :0.22129566152626035
AUPRC :0.4579666498635767

Faithfulness
Comprehensiveness :0.018521106214968816
Sufficiency :0.020726626371777544
0.10379374161152372	0.22129566152626035	0.4579666498635767	0.018521106214968816	0.020726626371777544
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Sun Apr  2 19:19:02 2023
End Sun Apr  2 19:37:19 2023

Plausibility
IOU F1 :0.0969296115573334
Token F1 :0.21681453758151192
AUPRC :0.4642085385151735

Faithfulness
Comprehensiveness :0.01853197828549896
Sufficiency :0.02092824834516632
0.0969296115573334	0.21681453758151192	0.4642085385151735	0.01853197828549896	0.02092824834516632
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Sun Apr  2 19:37:24 2023
End Sun Apr  2 19:43:03 2023

Plausibility
IOU F1 :0.12051315192905197
Token F1 :0.23291063850752894
AUPRC :0.49059486779898

Faithfulness
Comprehensiveness :-0.04011262458357589
Sufficiency :-0.04308964247073804
0.12051315192905197	0.23291063850752894	0.49059486779898	-0.04011262458357589	-0.04308964247073804
Tue Apr  4 13:55:43 2023
Keep-4 Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924


======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 500 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Fri Apr  7 02:17:14 2023
End Fri Apr  7 02:35:22 2023

Plausibility
IOU F1 :0.13927151299580032
Token F1 :0.23953826823733373
AUPRC :0.5122781912937118

Faithfulness
Comprehensiveness :-0.03592368436787942
Sufficiency :-0.044929737452546775
0.13927151299580032	0.23953826823733373	0.5122781912937118	-0.03592368436787942	-0.044929737452546775
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 50 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Fri Apr  7 01:53:58 2023
End Fri Apr  7 01:58:21 2023

Plausibility
IOU F1 :0.11720998848238898
Token F1 :0.22971925296187207
AUPRC :0.48592007161509443

Faithfulness
Comprehensiveness :-0.04091238991933472
Sufficiency :-0.043333035373700625
0.11720998848238898	0.22971925296187207	0.48592007161509443	-0.04091238991933472	-0.043333035373700625
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 10 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Fri Apr  7 02:22:51 2023
End Fri Apr  7 02:26:17 2023

Plausibility
IOU F1 :0.10845768683777926
Token F1 :0.22020239639639272
AUPRC :0.47435579592078914

Faithfulness
Comprehensiveness :-0.042391180745686076
Sufficiency :-0.04215718427884616
0.10845768683777926	0.22020239639639272	0.47435579592078914	-0.042391180745686076	-0.04215718427884616
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 1000 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Fri Apr  7 06:40:55 2023
End Fri Apr  7 07:14:53 2023

Plausibility
IOU F1 :0.15219545560383377
Token F1 :0.2488213186581807
AUPRC :0.5347102070305635

Faithfulness
Comprehensiveness :-0.03361998845524948
Sufficiency :-0.04604792746798336
0.15219545560383377	0.2488213186581807	0.5347102070305635	-0.03361998845524948	-0.04604792746798336
======= hatexplain metrics on: trained_models/runID-26-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_features 10 --lime_num_samples 1500 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-26-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen True 
Begin Fri Apr  7 22:27:21 2023
End Fri Apr  7 23:16:21 2023

Plausibility
IOU F1 :0.16538112088996967
Token F1 :0.24080103115680923
AUPRC :0.5393913248445957

Faithfulness
Comprehensiveness :-0.032176853698024956
Sufficiency :-0.04618579742619543
0.16538112088996967	0.24080103115680923	0.5393913248445957	-0.032176853698024956	-0.04618579742619543
Tue Apr 11 14:57:06 2023
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924


Tue Apr 11 15:39:42 2023
Testing Accuracy : 0.595
              precision    recall  f1-score   support

           0       1.00      0.00      0.01       782
           1       0.59      1.00      0.75      1142

    accuracy                           0.59      1924
   macro avg       0.80      0.50      0.38      1924
weighted avg       0.76      0.59      0.44      1924


Tue Apr 11 16:20:53 2023
Keep-k = 5
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924


Fri Apr 14 19:00:18 2023
Keep-k = 1
Testing Accuracy : 0.406
              precision    recall  f1-score   support

           0       0.41      1.00      0.58       782
           1       0.00      0.00      0.00      1142

    accuracy                           0.41      1924
   macro avg       0.20      0.50      0.29      1924
weighted avg       0.17      0.41      0.23      1924


Tue Apr 18 15:46:33 2023
Keep-k = 2
Testing Accuracy : 0.433
              precision    recall  f1-score   support

           0       0.42      0.98      0.58       782
           1       0.82      0.06      0.11      1142

    accuracy                           0.43      1924
   macro avg       0.62      0.52      0.35      1924
weighted avg       0.66      0.43      0.30      1924


Tue Apr 18 17:22:40 2023
Keep-k = 3
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.50      0.01      0.01       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.55      0.50      0.38      1924
weighted avg       0.56      0.59      0.45      1924


Thu Apr 20 21:29:49 2023
Keep-k = 4
Testing Accuracy : 0.539
              precision    recall  f1-score   support

           0       0.46      0.69      0.55       782
           1       0.67      0.44      0.53      1142

    accuracy                           0.54      1924
   macro avg       0.56      0.56      0.54      1924
weighted avg       0.58      0.54      0.54      1924


