=====================RUN ID:  20=======================
hatespeech-training.py --split 3 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen True --encoder_name roberta-base --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 20 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: roberta-base
encoder_frozen?: True
bias_in_fc?: True
cls_token?: False
Data split: 3
Thu Mar 23 19:33:18 2023

EPOCH: 1/10
Training Loss: 0.661, Training Accuracy : 0.601
Validation Loss: 0.690, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 2/10
Training Loss: 0.649, Training Accuracy : 0.618
Validation Loss: 0.678, Validation Accuracy : 0.593

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       781
           1       0.59      1.00      0.75      1141

    accuracy                           0.59      1922
   macro avg       0.30      0.50      0.37      1922
weighted avg       0.35      0.59      0.44      1922



EPOCH: 3/10
Training Loss: 0.636, Training Accuracy : 0.638
Validation Loss: 0.667, Validation Accuracy : 0.599

              precision    recall  f1-score   support

           0       1.00      0.01      0.02       781
           1       0.60      1.00      0.75      1141

    accuracy                           0.60      1922
   macro avg       0.80      0.50      0.38      1922
weighted avg       0.76      0.60      0.45      1922



EPOCH: 4/10
Training Loss: 0.625, Training Accuracy : 0.648
Validation Loss: 0.661, Validation Accuracy : 0.603

              precision    recall  f1-score   support

           0       0.90      0.02      0.04       781
           1       0.60      1.00      0.75      1141

    accuracy                           0.60      1922
   macro avg       0.75      0.51      0.40      1922
weighted avg       0.72      0.60      0.46      1922



EPOCH: 5/10
Training Loss: 0.620, Training Accuracy : 0.658
Validation Loss: 0.655, Validation Accuracy : 0.608

              precision    recall  f1-score   support

           0       0.84      0.04      0.08       781
           1       0.60      0.99      0.75      1141

    accuracy                           0.61      1922
   macro avg       0.72      0.52      0.41      1922
weighted avg       0.70      0.61      0.48      1922



EPOCH: 6/10
Training Loss: 0.615, Training Accuracy : 0.664
Validation Loss: 0.650, Validation Accuracy : 0.615

              precision    recall  f1-score   support

           0       0.83      0.06      0.11       781
           1       0.61      0.99      0.75      1141

    accuracy                           0.61      1922
   macro avg       0.72      0.53      0.43      1922
weighted avg       0.70      0.61      0.49      1922



EPOCH: 7/10
Training Loss: 0.612, Training Accuracy : 0.667
Validation Loss: 0.646, Validation Accuracy : 0.617

              precision    recall  f1-score   support

           0       0.76      0.08      0.14       781
           1       0.61      0.98      0.75      1141

    accuracy                           0.62      1922
   macro avg       0.69      0.53      0.45      1922
weighted avg       0.67      0.62      0.50      1922



EPOCH: 8/10
Training Loss: 0.612, Training Accuracy : 0.669
Validation Loss: 0.643, Validation Accuracy : 0.623

              precision    recall  f1-score   support

           0       0.77      0.10      0.18       781
           1       0.61      0.98      0.75      1141

    accuracy                           0.62      1922
   macro avg       0.69      0.54      0.47      1922
weighted avg       0.68      0.62      0.52      1922



EPOCH: 9/10
Training Loss: 0.614, Training Accuracy : 0.665
Validation Loss: 0.634, Validation Accuracy : 0.635

              precision    recall  f1-score   support

           0       0.73      0.15      0.26       781
           1       0.62      0.96      0.76      1141

    accuracy                           0.63      1922
   macro avg       0.68      0.56      0.51      1922
weighted avg       0.67      0.63      0.55      1922



EPOCH: 10/10
Training Loss: 0.627, Training Accuracy : 0.649
Validation Loss: 0.629, Validation Accuracy : 0.648

              precision    recall  f1-score   support

           0       0.61      0.38      0.46       781
           1       0.66      0.83      0.74      1141

    accuracy                           0.65      1922
   macro avg       0.63      0.60      0.60      1922
weighted avg       0.64      0.65      0.63      1922


Thu Mar 23 19:48:33 2023
Testing Accuracy : 0.648
              precision    recall  f1-score   support

           0       0.61      0.38      0.47       782
           1       0.66      0.83      0.74      1142

    accuracy                           0.65      1924
   macro avg       0.64      0.61      0.60      1924
weighted avg       0.64      0.65      0.63      1924


======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Sun Apr  2 16:32:09 2023
End Sun Apr  2 16:49:30 2023

Plausibility
IOU F1 :0.11162073594599858
Token F1 :0.220970671815157
AUPRC :0.4735656596079515

Faithfulness
Comprehensiveness :0.04350158387759874
Sufficiency :0.06362586590800416
0.11162073594599858	0.220970671815157	0.4735656596079515	0.04350158387759874	0.06362586590800416
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Sun Apr  2 16:49:35 2023
End Sun Apr  2 17:11:08 2023

Plausibility
IOU F1 :0.1311303760694573
Token F1 :0.22893943053440263
AUPRC :0.4781928084433616

Faithfulness
Comprehensiveness :0.049280264241528066
Sufficiency :0.0623298598108108
0.1311303760694573	0.22893943053440263	0.4781928084433616	0.049280264241528066	0.0623298598108108
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Sun Apr  2 17:11:13 2023
End Sun Apr  2 17:28:37 2023

Plausibility
IOU F1 :0.12453618964218757
Token F1 :0.2283627945979539
AUPRC :0.4763261851327606

Faithfulness
Comprehensiveness :0.045633794006808726
Sufficiency :0.06361908682307693
0.12453618964218757	0.2283627945979539	0.4763261851327606	0.045633794006808726	0.06361908682307693
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Sun Apr  2 17:28:42 2023
End Sun Apr  2 17:34:28 2023

Plausibility
IOU F1 :0.1296528976652718
Token F1 :0.2267826111084458
AUPRC :0.4970066101850862

Faithfulness
Comprehensiveness :-0.005087272495686071
Sufficiency :-0.0370680885762474
0.1296528976652718	0.2267826111084458	0.4970066101850862	-0.005087272495686071	-0.0370680885762474
Tue Apr  4 13:26:13 2023
Keep-4 Testing Accuracy : 0.599
              precision    recall  f1-score   support

           0       1.00      0.01      0.03       782
           1       0.60      1.00      0.75      1142

    accuracy                           0.60      1924
   macro avg       0.80      0.51      0.39      1924
weighted avg       0.76      0.60      0.45      1924


======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 500 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Fri Apr  7 01:40:39 2023
End Fri Apr  7 01:58:35 2023

Plausibility
IOU F1 :0.14543388142473218
Token F1 :0.20530350929548574
AUPRC :0.5072205202010469

Faithfulness
Comprehensiveness :0.0062477922795218304
Sufficiency :-0.04229443478601871
0.14543388142473218	0.20530350929548574	0.5072205202010469	0.0062477922795218304	-0.04229443478601871
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 50 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Fri Apr  7 01:44:54 2023
End Fri Apr  7 01:49:19 2023

Plausibility
IOU F1 :0.11332204033881972
Token F1 :0.21875032962934252
AUPRC :0.48486014417976947

Faithfulness
Comprehensiveness :-0.008246928318659042
Sufficiency :-0.03589452129386694
0.11332204033881972	0.21875032962934252	0.48486014417976947	-0.008246928318659042	-0.03589452129386694
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 10 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Fri Apr  7 02:15:58 2023
End Fri Apr  7 02:19:12 2023

Plausibility
IOU F1 :0.10661931441110374
Token F1 :0.22218002125770092
AUPRC :0.4742933406879569

Faithfulness
Comprehensiveness :-0.013832070062110187
Sufficiency :-0.033617628820374224
0.10661931441110374	0.22218002125770092	0.4742933406879569	-0.013832070062110187	-0.033617628820374224
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_samples 1000 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Fri Apr  7 05:33:16 2023
End Fri Apr  7 06:06:32 2023

Plausibility
IOU F1 :0.15178679907429876
Token F1 :0.20209955620465525
AUPRC :0.5102430668581039

Faithfulness
Comprehensiveness :0.010679321528846156
Sufficiency :-0.043702148583471934
0.15178679907429876	0.20209955620465525	0.5102430668581039	0.010679321528846156	-0.043702148583471934
======= hatexplain metrics on: trained_models/runID-20-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_features 10 --lime_num_samples 1500 --faithfullness_filtering top-k --split 3 --model_path trained_models/runID-20-checkpoint.pth --data_path data/ --encoder_name roberta-base --encoder_frozen True 
Begin Fri Apr  7 20:49:54 2023
End Fri Apr  7 21:37:55 2023

Plausibility
IOU F1 :0.14986459976568184
Token F1 :0.19367171131989055
AUPRC :0.5039267181083943

Faithfulness
Comprehensiveness :0.00994624587702703
Sufficiency :-0.03924785687681913
0.14986459976568184	0.19367171131989055	0.5039267181083943	0.00994624587702703	-0.03924785687681913
Tue Apr 11 14:46:42 2023
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924


Tue Apr 11 15:27:12 2023
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924


Tue Apr 11 16:10:16 2023
Keep-k = 5
Testing Accuracy : 0.604
              precision    recall  f1-score   support

           0       0.76      0.04      0.07       782
           1       0.60      0.99      0.75      1142

    accuracy                           0.60      1924
   macro avg       0.68      0.51      0.41      1924
weighted avg       0.67      0.60      0.47      1924


Fri Apr 14 12:29:08 2023
Keep-k = 1
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       782
           1       0.59      1.00      0.74      1142

    accuracy                           0.59      1924
   macro avg       0.30      0.50      0.37      1924
weighted avg       0.35      0.59      0.44      1924



Tue Apr 18 15:37:21 2023
Keep-k = 2
Testing Accuracy : 0.594
              precision    recall  f1-score   support

           0       1.00      0.00      0.00       782
           1       0.59      1.00      0.75      1142

    accuracy                           0.59      1924
   macro avg       0.80      0.50      0.37      1924
weighted avg       0.76      0.59      0.44      1924


Tue Apr 18 17:14:13 2023
Keep-k = 3
Testing Accuracy : 0.599
              precision    recall  f1-score   support

           0       1.00      0.01      0.02       782
           1       0.60      1.00      0.75      1142

    accuracy                           0.60      1924
   macro avg       0.80      0.51      0.38      1924
weighted avg       0.76      0.60      0.45      1924


Thu Apr 20 21:21:32 2023
Keep-k = 4
Testing Accuracy : 0.604
              precision    recall  f1-score   support

           0       0.79      0.03      0.07       782
           1       0.60      0.99      0.75      1142

    accuracy                           0.60      1924
   macro avg       0.70      0.51      0.41      1924
weighted avg       0.68      0.60      0.47      1924


