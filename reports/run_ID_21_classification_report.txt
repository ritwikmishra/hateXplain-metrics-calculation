=====================RUN ID:  21=======================
hatespeech-training.py --split 1 --bias_in_fc True --add_cls_sep_tokens False --epochs 10 --encoder_frozen False --encoder_name xlm-roberta-base --data_path data/ --checkpoint_path . --message same params as run id 2 --dummy False --run_ID 21 --drop_out 0.40 --bert_lr 5e-7 --ft_lr 1e-6 MESSAGE : same params as run id 2
FINE TUNING LAYERS: 
flat_dense): Linear(in_features=76800, out_features=768, bias=True)
  (relu1): ReLU()
  (fc1): Linear(in_features=768, out_features=128, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (relu2): ReLU()
  (fc2): Linear(in_features=128, out_features=2, bias=True)
  (log_softmax): LogSoftmax(dim=1)

Bert layers learning_rate: 5e-07finetuning Layers Learning rate: 1e-06encoder_name: xlm-roberta-base
encoder_frozen?: False
bias_in_fc?: True
cls_token?: False
Data split: 1
Thu Mar 23 21:12:13 2023

EPOCH: 1/10
Training Loss: 0.658, Training Accuracy : 0.566
Validation Loss: 0.707, Validation Accuracy : 0.587

              precision    recall  f1-score   support

           0       0.83      0.01      0.01       799
           1       0.59      1.00      0.74      1125

    accuracy                           0.59      1924
   macro avg       0.71      0.50      0.38      1924
weighted avg       0.69      0.59      0.44      1924



EPOCH: 2/10
Training Loss: 0.596, Training Accuracy : 0.667
Validation Loss: 0.663, Validation Accuracy : 0.645

              precision    recall  f1-score   support

           0       0.77      0.21      0.33       799
           1       0.63      0.95      0.76      1125

    accuracy                           0.65      1924
   macro avg       0.70      0.58      0.54      1924
weighted avg       0.69      0.65      0.58      1924



EPOCH: 3/10
Training Loss: 0.531, Training Accuracy : 0.735
Validation Loss: 0.654, Validation Accuracy : 0.666

              precision    recall  f1-score   support

           0       0.78      0.27      0.40       799
           1       0.65      0.94      0.77      1125

    accuracy                           0.67      1924
   macro avg       0.71      0.61      0.59      1924
weighted avg       0.70      0.67      0.62      1924



EPOCH: 4/10
Training Loss: 0.498, Training Accuracy : 0.754
Validation Loss: 0.633, Validation Accuracy : 0.684

              precision    recall  f1-score   support

           0       0.79      0.33      0.46       799
           1       0.66      0.94      0.78      1125

    accuracy                           0.68      1924
   macro avg       0.73      0.63      0.62      1924
weighted avg       0.72      0.68      0.65      1924



EPOCH: 5/10
Training Loss: 0.478, Training Accuracy : 0.766
Validation Loss: 0.614, Validation Accuracy : 0.698

              precision    recall  f1-score   support

           0       0.79      0.38      0.51       799
           1       0.68      0.93      0.78      1125

    accuracy                           0.70      1924
   macro avg       0.73      0.65      0.65      1924
weighted avg       0.72      0.70      0.67      1924



EPOCH: 6/10
Training Loss: 0.464, Training Accuracy : 0.774
Validation Loss: 0.605, Validation Accuracy : 0.704

              precision    recall  f1-score   support

           0       0.79      0.40      0.53       799
           1       0.68      0.92      0.79      1125

    accuracy                           0.70      1924
   macro avg       0.73      0.66      0.66      1924
weighted avg       0.73      0.70      0.68      1924



EPOCH: 7/10
Training Loss: 0.457, Training Accuracy : 0.780
Validation Loss: 0.601, Validation Accuracy : 0.709

              precision    recall  f1-score   support

           0       0.77      0.42      0.55       799
           1       0.69      0.91      0.79      1125

    accuracy                           0.71      1924
   macro avg       0.73      0.67      0.67      1924
weighted avg       0.72      0.71      0.69      1924



EPOCH: 8/10
Training Loss: 0.449, Training Accuracy : 0.785
Validation Loss: 0.588, Validation Accuracy : 0.718

              precision    recall  f1-score   support

           0       0.77      0.46      0.57       799
           1       0.70      0.90      0.79      1125

    accuracy                           0.72      1924
   macro avg       0.74      0.68      0.68      1924
weighted avg       0.73      0.72      0.70      1924



EPOCH: 9/10
Training Loss: 0.446, Training Accuracy : 0.788
Validation Loss: 0.576, Validation Accuracy : 0.727

              precision    recall  f1-score   support

           0       0.77      0.49      0.60       799
           1       0.71      0.90      0.79      1125

    accuracy                           0.73      1924
   macro avg       0.74      0.69      0.70      1924
weighted avg       0.74      0.73      0.71      1924



EPOCH: 10/10
Training Loss: 0.457, Training Accuracy : 0.782
Validation Loss: 0.533, Validation Accuracy : 0.741

              precision    recall  f1-score   support

           0       0.74      0.59      0.65       799
           1       0.74      0.85      0.79      1125

    accuracy                           0.74      1924
   macro avg       0.74      0.72      0.72      1924
weighted avg       0.74      0.74      0.73      1924


Thu Mar 23 22:00:35 2023
Testing Accuracy : 0.746
              precision    recall  f1-score   support

           0       0.75      0.57      0.65       789
           1       0.74      0.87      0.80      1133

    accuracy                           0.75      1922
   macro avg       0.75      0.72      0.72      1922
weighted avg       0.75      0.75      0.74      1922


======= hatexplain metrics on: trained_models/runID-21-checkpoint.pth==========
ajeet-calculate-metrics.py --method 0 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-21-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen False 
Begin Sat Mar 25 03:03:51 2023
End Sat Mar 25 03:20:23 2023

Plausibility
IOU F1 :0.12016697588796953
Token F1 :0.19640561518802163
AUPRC :0.4846737284172438

Faithfulness
Comprehensiveness :0.33631328029760665
Sufficiency :0.2915470190165973
0.12016697588796953	0.19640561518802163	0.4846737284172438	0.33631328029760665	0.2915470190165973
======= hatexplain metrics on: trained_models/runID-21-checkpoint.pth==========
ajeet-calculate-metrics.py --method 1 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-21-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen False 
Begin Sat Mar 25 03:20:28 2023
End Sat Mar 25 03:40:38 2023

Plausibility
IOU F1 :0.13237091989511007
Token F1 :0.1950681976182031
AUPRC :0.4850625515117674

Faithfulness
Comprehensiveness :0.3364849231472945
Sufficiency :0.2910656608933923
0.13237091989511007	0.1950681976182031	0.4850625515117674	0.3364849231472945	0.2910656608933923
======= hatexplain metrics on: trained_models/runID-21-checkpoint.pth==========
ajeet-calculate-metrics.py --method 2 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-21-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen False 
Begin Sat Mar 25 03:40:43 2023
End Sat Mar 25 03:57:08 2023

Plausibility
IOU F1 :0.1303608880990826
Token F1 :0.19440399861887453
AUPRC :0.4882957896362551

Faithfulness
Comprehensiveness :0.33536300238152966
Sufficiency :0.2941682116084287
0.1303608880990826	0.19440399861887453	0.4882957896362551	0.33536300238152966	0.2941682116084287
======= hatexplain metrics on: trained_models/runID-21-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-21-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen False 
Begin Sat Mar 25 03:57:13 2023
End Sat Mar 25 04:02:54 2023

Plausibility
IOU F1 :0.24982892589226438
Token F1 :0.28267619153107154
AUPRC :0.6299348414851882

Faithfulness
Comprehensiveness :0.010643865698231012
Sufficiency :-0.08372971380431843
0.24982892589226438	0.28267619153107154	0.6299348414851882	0.010643865698231012	-0.08372971380431843
Tue Apr  4 13:27:50 2023
Keep-4 Testing Accuracy : 0.728
              precision    recall  f1-score   support

           0       0.79      0.46      0.58       789
           1       0.71      0.91      0.80      1133

    accuracy                           0.73      1922
   macro avg       0.75      0.69      0.69      1922
weighted avg       0.74      0.73      0.71      1922


======= hatexplain metrics on: trained_models/runID-21-checkpoint.pth==========
ajeet-calculate-metrics.py --method lime --lime_num_features 10 --lime_num_samples 1500 --faithfullness_filtering top-k --split 1 --model_path trained_models/runID-21-checkpoint.pth --data_path data/ --encoder_name xlm-roberta-base --encoder_frozen False 
Begin Sat Apr  8 04:30:14 2023
End Sat Apr  8 05:20:48 2023

Plausibility
IOU F1 :0.26819308317334983
Token F1 :0.2753481716881016
AUPRC :0.6496693620192225

Faithfulness
Comprehensiveness :0.030357395621904266
Sufficiency :-0.055115374508428724
0.26819308317334983	0.2753481716881016	0.6496693620192225	0.030357395621904266	-0.055115374508428724
Tue Apr 11 14:48:11 2023
Testing Accuracy : 0.717
              precision    recall  f1-score   support

           0       0.78      0.43      0.55       789
           1       0.70      0.91      0.79      1133

    accuracy                           0.72      1922
   macro avg       0.74      0.67      0.67      1922
weighted avg       0.73      0.72      0.69      1922


Tue Apr 11 15:30:03 2023
Testing Accuracy : 0.734
              precision    recall  f1-score   support

           0       0.73      0.56      0.63       789
           1       0.73      0.86      0.79      1133

    accuracy                           0.73      1922
   macro avg       0.73      0.71      0.71      1922
weighted avg       0.73      0.73      0.73      1922


Tue Apr 11 16:11:47 2023
Keep-k = 5
Testing Accuracy : 0.731
              precision    recall  f1-score   support

           0       0.75      0.51      0.61       789
           1       0.72      0.88      0.79      1133

    accuracy                           0.73      1922
   macro avg       0.74      0.70      0.70      1922
weighted avg       0.73      0.73      0.72      1922


Fri Apr 14 18:53:04 2023
Keep-k = 1
Testing Accuracy : 0.632
              precision    recall  f1-score   support

           0       0.53      0.93      0.67       789
           1       0.89      0.42      0.57      1133

    accuracy                           0.63      1922
   macro avg       0.71      0.68      0.62      1922
weighted avg       0.74      0.63      0.62      1922


Tue Apr 18 15:38:59 2023
Keep-k = 2
Testing Accuracy : 0.729
              precision    recall  f1-score   support

           0       0.72      0.55      0.62       789
           1       0.73      0.85      0.79      1133

    accuracy                           0.73      1922
   macro avg       0.73      0.70      0.71      1922
weighted avg       0.73      0.73      0.72      1922


Tue Apr 18 17:15:36 2023
Keep-k = 3
Testing Accuracy : 0.710
              precision    recall  f1-score   support

           0       0.77      0.41      0.54       789
           1       0.69      0.92      0.79      1133

    accuracy                           0.71      1922
   macro avg       0.73      0.66      0.66      1922
weighted avg       0.72      0.71      0.68      1922


Thu Apr 20 21:22:53 2023
Keep-k = 4
Testing Accuracy : 0.734
              precision    recall  f1-score   support

           0       0.67      0.70      0.68       789
           1       0.78      0.76      0.77      1133

    accuracy                           0.73      1922
   macro avg       0.73      0.73      0.73      1922
weighted avg       0.74      0.73      0.73      1922


